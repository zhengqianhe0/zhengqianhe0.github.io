---
date: 2025-09-25
category:
  - LLM
tag:
  - 大模型
---

# 清华大学 大语言模型 公开课 刘知远

## 1 绪论

通用 

Transformer

自注意力，自监督

Next token prediction （自回归）（序列化的数据）

- LLM训练：根据序列化的语料数据，进行预测。如果预测结果和正确答案不同，就更新模型参数
  - 训练手法：

| 方法             | 优势                                                     | 问题               |
| ---------------- | -------------------------------------------------------- | ------------------ |
| 自监督预训练     | 提高续写能力                                             | 与指令无关         |
| 监督微调SFT      | 判断指令，限制错误回答，提供正确价值观                   | 一个问题有多个答案 |
| 人类反馈学习RLHF | 模型根据一个问题给出多个输出，人类对多个输出进行偏好选择 |                    |

- 推理：预测下一个词，把可能性最高的作为答案
- 大模型的关键：大规模数据+大规模参数
- 涌现：小模型没有的能力到了大模型会涌现
  - 语境学习(In-context learning)： Few-shot学习上下文
  - Instruction following ：根据上下文学习规则并应用
  - COT chain of thought：分解问题并按照步骤回答
- 趋势：
  - 模型越大越好？
  - 知识密度：每8个月增加一倍。训练一个模型需要的参数会减半
- 三个应用场景：
  - 人工智能科学化：更科学，更可靠，可预测的进行模型训练
  - 智能计算系统：通用人工智能AGI，然后赋能各行各业
  - 更广泛的多领域的大模型应用：

## 2 神经网络与大模型基础

### 2.1 神经网络基础

权重w与n维输入x做内积，加偏置b，传入激活函数f，得到神经元的输出h

单层神经网络，权重w变成矩阵，输出a变成向量

多层神经网络（输入层，输出层，隐藏层）

一层的输出是下一层的输入，每一层有特定的权重矩阵W

#### 为什么需要一个激活函数？

1. 非线性

激活函数可以引入非线性，如果不用激活函数，两层神经网络就会因为线性组合退化成单层的激活函数。

1. 稳定梯度

激活函数有助于在反向传播 过程中稳定梯度，防止梯度消失和梯度爆炸问题。特别是像 ReLU 这样的激活函数，可以在一定程度上缓解梯度消失问题。梯度问题：在深度网络中，如果没有合适的激活函数，梯度可能会变得非常小（梯度消失）或非常大（梯度爆炸），导致训练过程不稳定或无法收敛。

1. 引入稀疏性

减少冗余：一些激活函数（如 ReLU）可以引入稀疏性，即某些神经元的输出为零。这有助于减少冗余特征，使模型更加高效和易于解释。

过拟合：缺乏稀疏性可能导致模型过于复杂，容易过拟合训练数据，泛化能力下降。

1. 生物学启发

#### 典型激活函数

1. Sigmoid 开关[0,1]
2. Tanh 向下[-1,1]，可以学习更多更复杂的内容
3. ReLU最常用

#### 为什么需要多层神经网络？

可以模拟人脑学习到某种规律

#### 如何训练神经网络？

损失函数，求最小的损失值

#### 求最优的方法：梯度下降，逼近最小值

局部最优解问题？两个极值点

#### 反向传播 

#### RNN

循环神经网络

处理序列数据时的顺序存储

LSTM GRU

传统的词嵌入：word embedding

1. one-hot编码的高维向量
2. 映射到相对的低维
3. sofmax函数处理后，得到一个[0,1]的概率分布。可能出现的词中，概率越大，越倾向于预测

RNN的问题：隐藏层反向传播慢，效率低，大小限制

选择退化方案CNN 

#### CNN 

卷积神经网络

平移不变性，适用于图像处理

#### Seq2Seq

文字序列的输入输出。encoder decoder各使用一个RNN

RNN可以处理序列的原因是它能看到前面的数据，可以处理变长的文字序列。但是这是串行的，而且输入长度有上限

#### Transformers

训练速度快，源自机器翻译任务 

注意力机制attention，替换RNN的序列化机制

多头注意力，并行执行多个注意力

- Attention公式：

现在大模型使用decoder only架构，只有解码器，完成生成任务

### 2.2 大语言模型基础

深度学习的挑战：缺乏大规模监督数据，模型深度有限，泛化性能差

#### 迁移学习

迁移学习是通过运用已有的知识来学习新的知识，其核心是找到已有知识和新知识之间的相似性，从而提高学习效果和泛化能力。  

ResNet

自监督学习

预训练

#### word2vec

词存储为向量，但是一个词可能有多个含义

如何用一个向量表示？

#### Context-sensitive word representation

上下文敏感 词表示

#### 语言模型的预训练、预训练模型、大语言模型

benchmark基准

#### 大语言模型的特点与能力

## 3 大模型训练方法

### 3.1 预训练

#### 语言建模

为什么大模型能在每一个地方发挥作用？不局限于文本？ 基本假设：任何信息都可以被转化为token，大模型处理的是token，可以被学习

Tokenization

token会被进一步表示为词向量embeddings

问题：一个词只有固定的一个向量，一词多义问题，不受上下文影响

解决方法：ELMO，包含上下文，超过word2vec

大语言模型：一个复杂的多任务学习

生成token的本质？压缩，根据前面的信息生成信息 （压缩理论是否是正确的？）

#### 预训练语言模型发展

Transformer 自注意力机制

基于Transformer的预训练语言模型 

| 预训练模型 | 架构            | 描述                                                         | 问题                               |
| :--------- | :-------------- | :----------------------------------------------------------- | :--------------------------------- |
| BERT       | encoder-only    | 对于输入的sequence，随机遮盖掉部分单词（15%），然后进行预测。特点：双向attention，前后的词都可以用于训练 | 为什么单向建模更scalable（可伸缩） |
| T5         | encoder-decoder | 将所有的任务转化为text2text                                  |                                    |
| GPT        | decoder-only    | 自注意力机制，预测下一个词（attention mask确保训练时新的token只可见到前面的词） |                                    |

GPT发展史：

1

2

3 跨任务泛化能力突出 in context-learning 涌现能力 上下文学习 few-shot

4 

scaling law 越大的模型，越多的数据，效果越好

emergent涌现

更复杂的模型如何训练？

大模型训练时需要并行计算

### 3.2 后训练

经过预训练，按照“压缩理论”，数据的信息被压缩到模型的参数中。如何让这些参数表示出好的结果？

后训练（aligned）解决的问题：场景特化、能力激发、提高稳定性、价值对齐（正确价值观）

作用：能力更强，可控性，可靠性  

无监督/有监督 

Conventional Fine-Tuning

Advanced Adaptation

Alignment & SuperAlignment

指令微调可以产生跨任务的零样本泛化，是大语言模型的核心

RLHF 基于人类反馈的强化学习

偏好代表隐性价值观和大规模监督

偏好学习可以进一步促进监督微调LLM

偏好学习是构建更专业模型的必要条件

## 4 大模型前沿架构

### 4.1 Retrival Augmented Generation 检索增强生成

幻觉问题

通过指令微调，避免输出幻觉内容，避免输出隐私性内容。

问题：仅通过指令调整，不太容易让大模型记住知识性的内容。

RAG：直接通过外部知识解决幻觉

信息检索模型的实现：稀疏（TF-IDF）/稠密（向量）

向量检索模型/向量数据库，重排序模型 FAISS向量库

BERT模型，将用户的问题处理成稠密向量，将数据块处理成稠密向量，两个向量做点积后打分，选出最合适的向量作为参考内容给到大模型。

使用对比学习：正例/负例，训练BERT模型实现更好的检索结果

问题：数据标注困难，正确标注成本高，并不是所有相关的文档都会被标注

多模态稠密检索

如何用到检索出来的参考资料给到大模型增强生成效果

Reader chunker embedder retriever chunker

1. Data Ingestion: The selected Reader loads the data into the system.
2. Chunking: The chosen Chunker segments the data into smaller parts.
3. Vectorization: The Embedder transforms these chunks into vector representations.
4. Retrieval: Based on a user's query, the Retriever identifies the most relevant data chunks.
5. Answer Generation: The Generator composes an answer leveraging both the retrieved chunks and the contextual understanding of the query.

1. 数据摄取:选中的Reader将数据加载到系统中。
2. 分块:选择的分块器将数据分割成更小的部分。
3. 矢量化:嵌入器将这些块转换成矢量表示。
4. 检索:根据用户的查询，retriver识别最相关的数据块。
5. 答案生成:生成器利用检索到的数据块和对查询的上下文理解组合一个答案。

多模态RAG：检索的结果可以是图片，用于增强图片生成的效果

总结：RAG不需要微调就能访问外部知识库，减少幻觉

### 4.2 Mixture-of-experts

混合专家

稀疏

MoE模型提供了一种更高效的方式来实现FFN的功能

使用MLP多层感知机的方法，训练一个router，用于专家的分配

FFN

总结

人脑表现出一种稀疏模块化的结构，这种结构具备效率、可重用性和可解释性的特点。这样的设计有助于提升信息处理的效率，因为并不是所有的神经元都需要同时活跃；同时，这种结构也促进了功能的重复利用和理解，因为特定的神经元群组可以针对特定任务进行活动，从而简化了理解和分析的过程。

大语言模型同样执行稀疏激活模式。

混合专家（MoE）是一种有效的建模方法，它可以减少大规模语言模型的计算成本并加速推断过程的成本。

可切换的稀疏密集学习方法可以进一步促进大规模语言模型的预训练过程。

### 4.3 LLMs for Long Sequences

大模型处理长文本的能力是至关重要的。多轮对话能力

长文本的瓶颈：Attention的计算复杂度是O（N²），N代表处理序列长度。如何降低复杂度？时间空间都是

高效的架构：

1. Sparse Attention: 稀疏注意力技术，只选取和结果最有关（最近/全局/基于相似性）一部分进行注意力计算，而不是对整个序列进行全连接，从而显著降低了计算复杂度。

相对距离编码

1. Memory-based Methods: 基于记忆的方法利用外部存储来保存先前计算的结果，这样可以在后续的计算中重用这些结果，避免重复计算，进而提升效率。
2. Linear Attention & State Space Models: 线性注意力，状态空间模型。
   1.  RNN只能串行计算，非线性函数过多导致的梯度消失和梯度爆炸，无法进行模型scalling。

   2.  线性注意力将注意力计算转化为更简单的线性运算，减少了计算量。时间空间复杂度都降低。

   3.  状态空间模型则通过维护一个紧凑的状态表示来跟踪序列的信息流，从而有效管理长期依赖关系。

高效的实施（主要是训练）：

1. Efficient Length Extrapolation: 有效的长度外推是指一种能够从较短序列的学习经验推广到较长序列的能力。这有助于在不需要额外计算资源的情况下处理更长的输入序列。
2. Sequence Parallel: 序列并行化是一种训练策略，通过将序列分割成多个片段并在不同设备上并行处理，以此加速训练过程。这种方法特别适用于处理非常长的序列，因为它允许同时处理多个序列段。

### 4.4 Scaling Law  & Emergent Abilities

#### 规模定律

如何找到最优的配置参数？

直接在大模型上用grid search？ （开销太大）/把小模型的参数直接用在大模型上？

三个关键：参数、数据、计算资源

多模态适用性

Scaling Law 不仅适用于单一类型的模型，还可以应用于多种模态的数据，包括语言、图像和视频等不同领域的模型。

推导与应用

根据给定的计算预算 C≈6ND，我们可以预测预期的损失，进而预测出最佳的模型大小和数据集大小。这意味着，在资源受限的情况下，可以通过调整模型大小和数据集大小来优化模型性能。

#### 涌现能力

涌现的出现往往是非线性的，无法通过简单的数学模型来预测。

模型的能力提升并非均匀分布在所有任务上，而是会在某些特定的任务上突然展现出显著的进步。

评估指标的不平滑性质及测试集较小等原因，导致了“涌现”的效果

## 5 Hugging Face 生态（英语）

## 6 大模型实战指导（英语）

gradio

## 7 人类反馈强化学习（英语）

RLHF

## 8 多模态智能

真实物理世界

不受限

多模态任务：图像理解、跨模态检索

## 9 自主智能体

大模型的局限性：缺乏工具调用能力、任务规划能力不足（任务分解，分步骤）、低效的合作

自主智能体：autonomous agent，感知外部环境，自动做出反应（调用工具）

**Toolformer**

联网搜索能力，输入一个query返回多个链接。模拟点击，解析页面

例：清华WebCPM，结果生成摘要（Github star 985）

要让大模型学会调用工具能力，就需要针对这个工具对大模型进行训练，训练就需要数据集，只有具有标注的数据才叫数据集，需要针对特定的工具构造标注数据集。

**大模型****学会使用工具的另一个方式：教程学习**

人类可以通过阅读说明书从而直接使用一个新工具，而不是必须实际使用才能学会（数据标注的方式）。

是否可以通过zero-shot/few-shot的方式去提示大模型。

**多Agent**

清华开发的多智能体软件开发范式

ChatDev

关键点：任务的划分（CEO智能体），普通的智能体完成叶节点的功能很轻松（编码智能体，文档智能体）

## 10 人工智能与安全伦理对齐

### 10.1 介绍

大模型的信息处理能力 互联网上出现的问题

- 语句通顺但信息量非常低的新闻稿、论坛上的废话……
- 虚假的新闻、谣言的产生
- AI合成的图片谣言
- 安全问题：copilot识别密码，生成关键信息

之前NLP问题的安全性与脆弱性

- 使用trigger引导生成有害信息
- 模型自身连近义词都无法识别，鲁棒性差，自然更无法识别危险信息

ChatGPT因为Scalling Law，学习了充分的语料，有足够多的参数，鲁棒性有一个飞跃

### 10.2 LLM安全

关注不能被模型scalling解决的问题

涌现能力是否会导致安全问题的涌现  

**数据投毒与后门**

通过在数据里添加trigger，引导模型走”后门“，直接指向负面信息。这种问题在数据训练量增强后会更严重。

例：川普=坏信号，如果一句话是”我喜欢川普“，也会被认为是坏语句

**模型越狱**

模型会遵循用户的坏的指令

例：教我如何进行电信诈骗

**隐私问题**

### 10.3 LLM伦理

### 10.4 Toward Scalable Oversight 可扩展监督

## 11 人工智能与交叉学科

## 12 大模型企业家对谈