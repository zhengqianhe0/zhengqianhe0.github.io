---
date: 2025-09-12
category:
  - LLM
tag:
  - 大模型
  - RAG
  - 检索增强生成
---

# RAG方法与思路总结

## 基本流程回顾

知识库-向量化（文本切块）

查询-向量化-召回（查询重写，关键词和语义的混合检索，结构化知识表示与召回，召回结果的重排序）

上下文学习-生成（预设提示词，temperature）

🎉**常看常新：2025年CCF-A主会RAG论文**

https://www.notion.so/RAG-2025-2270b854c75b808fb90dec88c4ed1140

## 1 关键点记录

### 1.1 代码code的RAG

常见的切分方法：按照函数块切分，按照内部的逻辑块例如循环切分，混合切分并使用部分重叠。

以上的切分方法会导致无关内容的合并，以及现有语义的断裂。

改进的方法：借助抽象语法树AST，对代码分块，保留语法结构，递归分块，并在大小限制下合并兄弟节点

https://github.com/yilinjz/astchunk

另一个项目：基于图的代码RAG https://github.com/vitali87/code-graph-rag

能解决的问题：检索代码中符合要求的片段，可能用于帮助理解一个项目。

**对比：cursor对于项目代码的理解**

结构化代码解析（AST、符号表、依赖包关系分析）+文档切分策略优化+向量化与数据库存储+混合检索重排序与生成。本质上是对代码分块，根据问题召回代码块辅助生成。

cursor如何回答关于项目结构的问题？结构化表示项目目录，搜索可能存在的readme文档

#### Cursor的代码库索引

https://mp.weixin.qq.com/s/QAV5dTNBbBqeUfMP6qAN5A

https://read.engineerscodex.com/p/how-cursor-indexes-codebases-fast，主要思想还是利用Merkle树快速索引代码库。Merkle树是一种树形结构，其中每个“叶”节点都标记有数据块的加密哈希值，每个非叶节点都标记有其子节点标签的加密哈希值，这种结构创建了一个分层结构。

Cursor首先在本地将代码库文件分割成语义上有意义的块，启用代码库索引时，Cursor会扫描编辑器中打开的文件夹，并计算所有有效文件的哈希值的Merkle树。代码块被发送到Cursor的服务器后，会使用OpenAI的嵌入API或自定义嵌入模型生成嵌入。但是，还是老问题，代码库索引的有效性在很大程度上取决于代码是如何被分割的。

在工具上，可以使用像tree-sitter这样的工具来进行AST解析，它支持多种编程语言。

《**[代码RAG第二弹：代码类的GraphRAG怎么做？一个示例项目](https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648421306&idx=1&sn=ac93f33cc8e1aa6a9b1722c2534b2554&scene=21#wechat_redirect)**》

### 1.2 多模态GraphRAG

https://mp.weixin.qq.com/s/UtSjX_D2k-Cp7iJT-CNqtQ

创新点：多模态大模型用于数据合成和数据增强（对现有数据集的裁剪、旋转等）

以文档为中心的多模态GraphRAG，依赖于底层知识库MultimodalDocGraph。文档场景下的问题：引用“见表1”需要链接当前文本块和图表，可以采用知识图谱的实体链接。因此，需要将知识图谱拓展到多模态元素。实现过程中，图片，表格，公式都可以被进一步文本化。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/fUBU1yiaEmJj1lNlicSZMWmdunBBGyCt7ayTf8HIFFFbMLjSISibGxY9daGpdWKg95libNI5adnrMCCVibmkc2gBbPQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

开源项目实现 https://github.com/HKUDS/RAG-Anything

代码实现效果：

- 文档处理方式不合理，速度慢：所有的类型的文档都可以接受，统一转成IMG和PDF（即使是纯文本），并借助MinerU进行处理。图像、表格、方程分别用不同的方法解析，图像统一base64编码。
- 实体与图像实体存在一对多关系，容易造成较大的噪声；
- 框架较大，难评用处大小

图谱构建过程：

1. chunk切分
2. 文本chunk使用命名实体识别、关系抽取等提取关键信息，构造**<entity, rel, entity>**
3. 图像等作为模态实体（modal_entity），借助视觉模型分析文本与图像的关系，建立连接
   1. **`<entity, belongs_to, modal_entity>`** 文本与模态实体的关联
   2. **`<chunk, related_to, modal_entity>`** chunk与模态实体
   3. **`<entity, locate_in, chunk>`** 实体与chunk
4. 最终图谱包括三类节点，以及以上四个主要关系边

**文档为中心的多模态GraphRAG及MultimodalDocGraph是个很好的故事，这个可以讲一个比较好的故事，多模态、kg、文档解析、rag都通了。😊**



类似的论文（河南大学，未开源）

https://arxiv.org/pdf/2509.10467

设计了一个复杂的工作流，处理文档的逻辑结构（directory），一级多模态元素（文本/图像/表格）

#### 构建图的多种思路

可以从实体触发，可以从query出发，也可以从chunk出发。

《**Query-Centric Graph Retrieval Augmented Generation**》(https://arxiv.org/pdf/2509.21237) 华为团队

核心的思路是**Doc2Query**生成基于文本块的查询-答案对，然后**构建查询中心图**（QCG），最后采用**多跳检索机制**从图中筛选相关文本块，最终送LLM做生成。

问题在于应用场景十分受限，每个chunk都生成QA对，计算量和索引量都非常大。

### 1.3 Deep Research进展

商业与非商业的实现的总结 https://arxiv.org/pdf/2506.12594

现有的开源agent框架 https://github.com/scienceaix/deepresearch

涉及到的组成部分：

- 基座与推理模型（上下文管理，记忆，CoT，ToT）
- 任务规划与执行（任务分解，层级规划，自动执行与监控，多agent协同）
- 工具使用与环境交互（网页导航与交互，内容处理，API调用，领域特定tool使用）
- 知识生成（结果评估，来源认证，生成结构化报告，交互）

实现架构：

1. 单体式，以核心推理引擎集中控制
2. 流水线式，定义严格的工作流和各阶段之间的接口与数据格式
3. 多智能体式，设计明确的通信协议，进行不同角色和责任的多智能体协作
4. 混合模式

评估要点：基础模型，推理效率，上下文长度，工具集成与环境适应性，任务规划与执行稳定性，知识聚合与输出质量，不同场景的适应性（学术研究，企业决策，个人知识管理）

待解决的问题：隐私，知识产权，可访问性

未来研究方向：高级推理架构，多模态集成，领域特化，基于人机协作的标准化

### 1.4 Embedding模型进展

https://mp.weixin.qq.com/s/HYd2EkU01O3IgQn2ENaEkw

传统embedding模型固定向量维度输出，参数量小。

从Nvidia的NV-Embed到jina-v4，embedding模型主逐渐转向：

- 单模态-多模态
- 单一向量-多向量
- 固定维度-自定义维度
- 小模型-大模型

并且，开始使用合成数据等方式挖掘难样本，并针对特定任务补充能力项。

**对比Qwen3系列embedding模型和reranker模型：**

|              | Embedding              | Reranker                                  |
| ------------ | ---------------------- | ----------------------------------------- |
| 处理数据类型 | 单个文本转化为语义向量 | 文本对（查询和文档），输出相关性分数      |
| 模型架构     | 双编码器架构           | 交叉编码器，对query和文档联合编码（极慢） |
| 技术基础     | 基于大语言模型         | 基于大语言模型                            |

训练过程：构造数据-lora微调-模型合并

RAG流程：

- 预处理：知识库切块，利用embedding模型向量化
- RAG：问题向量化，余弦相似度匹配初步召回，重排序top-n，合并查询上下文构造prompt，生成

#### 一个趋势：小参数的embedding模型

Google的Embedding Gemma-308M模型，支持100+语言，输出嵌入维度大小可选。

3.08亿个参数，1亿模型参数，2亿嵌入参数。

- 嵌入参数指的是“词表大小V×嵌入维度D”
  - 嵌入维度指的是输入嵌入的维度，即初始文本的每个token初步处理成高维向量时的维度

⚠**目前尺寸更长、维度可定义、模型更小、数据更多样化、特定任务特定instruction的趋势逐步成为标配**。

**使用案例**：

1. 文本
2. 分词token
3. 初始嵌入
4. transformer捕捉上下文关系生成深层表示
5. 对token级向量均值池化（特征压缩），聚合成单一的上下文向量。原始每个token都有一个1×768维的向量表示，通过按句子压缩，每一个token的同一维度被聚合，得到对于句子的1×768维向量。好处：高校、防止过拟合，无论句子有多少token向量表示维度都一致。
6. 通过全连接层生成指定的输出维度

小模型应用场景：高响应速度需求、端侧部署

### 1.5 文档智能专题

https://mp.weixin.qq.com/s/QAV5dTNBbBqeUfMP6qAN5A

文档解析可以使用单独的多模态大模型进行解析，也可以构建工作流，进行布局分析，文本提取。继续做差异性可以在于：跨页图表/段落的合并。跨页文档，甚至有专门的数据集由于评测。需要判断以下问题：

- 文档问题回答失败的核心，是否是跨页导致的
- 为了解决跨页问题导致的时间开销是否有必要

过去的典型项目：https://mp.weixin.qq.com/s/5852Kn8wVlsSGpclrjkBNA，涉及RAGflow（企业级领域知识库RAG问答工作流搭建解决方案），MinerU（专长于PDF分析）等， 提供了很多细节的文档解析功能。

#### 数据构建

文档解析模型，布局检测、多模态解析模型，都可以用到大模型训练语料的处理当中。

https://mp.weixin.qq.com/s/idPm-boNEsZtMaNOXJJe9A

https://huggingface.co/datasets/HuggingFaceFW/finepdfs

构造过程中集成了OCR，布局检测，语言识别，hash去重，姓名隐私化处理等，还训练了单独的xgboost模型用于需求检测与分类。

#### 面向文档布局优化的多模态文档

《**Logics-Parsing: An End-to-end Document Parsing Model with Layout-centric Reinforcement Learning**》，https://github.com/alibaba/Logics-Parsing，https://arxiv.org/pdf/2509.19760

在解决复杂文档问答问题时，对于报纸、poster等多列多图的多模态资源，布局分析与阅读顺序是优化的一个方向。

采用：

面向多任务的SFT（布局分析、内容提取、分类、逻辑解析）

+面向布局的RL（文字提取效果、布局分析准确率、阅读顺序）

#### 多模态长文档RAG

https://mp.weixin.qq.com/s/O6qWnWJ9HnfaYDTM3DKGUQ

##### 检索技术

《A Survey of Long-Document Retrieval in the PLM and LLM Era》，https://arxiv.org/pdf/2509.07759 苏州大学

##### benchmark

《**VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding**》，https://arxiv.org/pdf/2508.07493，https://github.com/puar-playground/VisR-Bench

建模真实场景中多语言、多页的长文档检索任务，整理了常见的相关数据集与基线模型

### 1.6 多模态RAG

https://mp.weixin.qq.com/s/BbT0XCGbjwJ6mXb2EuVyGg

做多模态RAG需要有多模态Embedding模型，例如ColBERT（本身是文本embed），ColPali。

以上的传统解决方案存在局限性：

- 检索结果是页面级的，没法分析页面的具体内容
- ColBERT依赖于文本信息，对文本中的数值信息解析能力差
- 框架M3DocRAG，能够结合文本和图像信息，但缺乏信息的细致提取，和跨模态整合能力

因此，尝试使用agent解决，https://arxiv.org/pdf/2503.13964，https://github.com/aiming-lab/MDocAgent，多模态多智能体框架，利用文本和图像信息来提高文档问答的准确性。

**MDocAgent**：

- 设计五个由prompt驱动的agent（初步生成，提取关键信息，文本处理，图像处理，综合智能体整合），**ColBERTv2和ColPali**分别作为文本和图像检索器
- 文档智能方面，还是使用OCR+PDF解析提取文本，按页保存为图像，生成文本和视觉表示
- 实现过程中，分别用两个检索器进行检索，agent各自分工处理文本和多模态，最终整合

#### 评估数据集

##### Double-Bench

https://arxiv.org/pdf/2508.03644

多模态文档检索数据集，单跳/多跳，多语言，多类文档

数据构建过程是亮点：：原始文档经过粗粒过滤（10-50页，使用GPT-4o进行语言验证），然后使用Docling和MinerU等工具进行模态分解，将每页拆分为构成文本、表格和图形组件，然后生成单跳查询，同时额外构建知识图谱，以辅助多跳查询的生成。这个点值得借鉴，**做多跳数据生成，还是使用知识图谱来做中间辅助。**

很棒的一篇多模态数据集工作：

- embedding模型选型（文本+多模态）
- MLLM选型
- 评测了文档多模态RAG框架（**MDocAgent**、**ViDoRAG**、**M3DOCRAG**、**Colqwen-gen**）

⚠**框架的回答准确性与检索准确性高度相关**，Colqwen-gen（强检索+简单生成）的多跳回答正确率接近复杂框架MDocAgent，证明“优化检索阶段”比“设计复杂生成流程”更关键；

**主流框架（如MDocAgent、ViDoRAG）倾向“有问必答”**，即便未检索到证据，仍生成推测性内容，过度自信。复杂框架（MDocAgent、ViDoRAG）因多智能体串行协调，推理时间是Colqwen-gen的4倍左右。

#### 一个小专题：非标准印刷体/问题图像的文档解析

几何弯曲、阴影、污渍等变化直接影响layout与ocr的效果

前沿的方案可以看《**DocRes: A Generalist Model Toward Unifying Document Image Restoration Task**s》，代码在：https://github.com/ZZZHANG-jx/DocRes/tree/master，https://arxiv.org/pdf/2405.04408

其意义在于提出一个统一了**五种文档图像还原任务的通用模型，包括去扭曲、去阴影、外观增强、去模糊和二值化**。<u>方法上，采用自行构建的一个模型。</u>

其他图像增强的文档处理整合

https://github.com/ZZZHANG-jx/Recommendations-Document-Image-Processing

### 1.7 prompt工程

RAG落地过程中prompt也是很重要的一环。

https://github.com/asgeirtj/system_prompts_leaks/  各大主流大模型的系统提示词

提示词的历史发展：

- zero-shot-CoT： Let's think step by step
- optimizers： Take a deep breath and work on this problem step-by-step
- emotion prompt： Are you sure？ This is very important to me.
  - PUA类：你确定？相信你的能力。你要将这个困难挑战视为成长机会

#### 腾讯混元 PromptEnhancer

https://www.arxiv.org/pdf/2509.04545

针对性的训练一个文生图prompt改写的模型

1. 模型蒸馏：构造高质量训练集图片，先生成简短caption，再借助Gemini生成CoT和多个候选prompt结果
2. 训练阶段，先做SFT，再做GRPO强化

评估不好做，拉维度来凑，所以设计了一个**包含24个细粒度关键点的分类体系，关键点被组织成六个主要类别**，涵盖包括：**语言和语法理解**，评估对否定等核心语言结构的解释；**视觉属性**，关注对象数量等渲染属性；**动作和互动**，衡量动态状态的表现；关系和组合结构，评估复杂场景的处理；以及由**知识和想象力以及图像内文本和布局所涵盖的更高层次能力**。

总结：**无论是做改写，还是做生成，都是需要有反馈，有reward，先sft，后grpo做强化，是目前常用策略**。

### 1.8 Memory上下文管理

对于agent，常见的上下文管理包括：

- 短期记忆：系统提示词+顺序存储完整上下文+用户问题
- 长期记忆：控制聊天记录长度做为短期内存，单独存储长期内存进行管理并检索相关记忆（效率会变低，但可以进行用户定制，跨越多个对话和历史对话）

- update类：针对memory管理使用curd操作。没出现过的加入，出现过有矛盾的要删除旧的，同一方向有补充的要更新，本质上是基于语义做RAG。如果还不够，在做知识图谱的实体与关系抽取。例子参考mem0：http://arxiv.org/pdf/2504.19413

记忆管理的榜单：https://github.com/NevaMind-AI/memU memu方法准确度最高，但单词检索需要1s，更不用说需要把检索结果用于大模型再生成内容。

### 1.9 RAG+科研：UltraRAG / FlashRAG

https://mp.weixin.qq.com/s/LEtuEKtZyiUqd1pdHW1XBA

https://github.com/OpenBMB/UltraRAG

定位：基于MCP架构设计的 “面向科研的RAG实验加速器”

使用方法：利用现有的组件，编写yaml配置文件，实现串行并行，分支循环，内置常用数据集

https://github.com/RUC-NLPIR/FlashRAG

定位：快速复现现有RAG框架

使用方法：可视化界面操作

### 1.10 GraphRAG的应用与落地

集成式RAG框架，https://github.com/apecloud/ApeRAG/ 在lightRAG（实体抽取、关系抽取、实体合并）基础上，使用了向量搜索和全文关键词搜索，并且以MinerU辅助进行文档解析。整体上是工程的优化，例如实现了知识图谱的隔离，可以做多租户。

GraphRAG落地的工程难题：

- 实体关系抽取时需要用LLM对文本块逐个调用大模型，而且初步抽取后可能还需要动态判断是否需要补充
- 知识图谱的隔离管理
- 实体合并与去重的准确性挑战，需要强大的上下文理解能力，尤其在专业领域内不能出错
- 依赖项较多：MinerU，LLM，图数据库，向量数据库，全文搜索引擎

#### 专题：知识抽取时的schema

Schema 指的是对图中实体（Entity）、关系（Relation）和属性（Attribute）的预先定义的结构化模式或本体（Ontology）。抽取前，事先明确定义好允许的实体类别（如 Person, Organization, Product）和关系类型（如 works_for, located_in, develops），抽取过程必须遵循这个预设框架。

因此，schema是一个工程化的解决方案。如果由专家预设的schema，覆盖足够全，则效果很好。但需要专家人工检查的成本，如果漏掉关键信息无法补充。

为什么主流GraphRAG（如LightRAG）选择“无Schema”？因为graphrag中的kg，其实作用做bridge的作用，也就是做锚点用，所以要尽可能的多，也无需要太多限制。GraphRAG并不追求构建一个完美的、可用于独立问答的知识库，而是服务于**下游检索任务**。只要这些节点能帮助用户从A跳到C（即使B有点噪声），它的目的就达到了。

### 1.11 RAG加速

Meta的工作，REFRAG（REpresentation For RAG）RAG解码框架，用了一个“压缩-感知-扩展”策略减少冗余计算，为工程策略。

### 1.12 Chunk切分策略

Is Semantic Chunking Worth the Computational Cost?  https://arxiv.org/pdf/2410.13070

论文通过实验证明了语义分块并不一定比固定分块效果好。

技术总结：https://weaviate.io/blog/chunking-strategies-for-rag

主要分为两大类：

- 预分块：向量化之前，对原始文档分块，提前决定块的大小和边界。优点在于提前建立好了索引，检索时效率高。
- 后分块：对于整个文档进行嵌入，在接收到查询时再对文档分块。通过缓存机制，存储分块结果，因此查询可以有动态性，分块策略具有上下文感知能力，但是效率相对低。

通用分块策略：

1. 固定大小：基于token分块，设置max-token，通过重叠overlap缓解完整性
2. 递归分块：基于现有的自然分隔符分块，如果有的块过长则递归分成更小的
3. 基于文档结构：按照标题、章节、段落、代码块等
4. 语义分块：先分句，然后按照嵌入向量的余弦相似度，相邻的句子如果高于相似度阈值则分为一块。
5. 大模型分块：直接让大模型把文本编程list。需要较长的上下文窗口
6. agentic分块：动态路由，根据文章特点选择不同的分块策略
7. 后分块：先嵌入文档，块的嵌入表示=块内的token的平均值。理想状况下，每个块的表示都代表了整个文档的上下文。
8. 层次分块：和文档结构很像，但更需要层次的依赖关系，不只依赖文件格式（第一段，第二段），还依赖章节结构（摘要，引言，方法）
9. adaptive：根据文档的内容动态调整参数（如chunk大小，overlap大小）。需要专门的模型判断，较少用

### 1.13 RAG+信息论

结合信息论计算RAG召回过程中的理论上界（数学基础）。目标：召回的top-k，定这个k，使得总信息量最大

**Chunk相关性最大化的动态TOP-K策略** https://arxiv.org/pdf/2509.04820

#### **信息增益** 

摸到了应用中基于信息论评估的边界，但仍缺少严格的形式化证明。具体的AAAI审稿时一篇关于文档噪声的证明可借鉴

https://arxiv.org/pdf/2509.12765

核心思想：相似度高的不一定对于生成结果有用，以信息量训练一个重排序模型

- 定义量化指标DIG：量化检索文档对正确答案生成的贡献，通过计算“有无该文档时LLM生成置信度的差值”（结合查询x与文档di时，LLM生成正确答案y的置信度，减去仅基于查询x时，LLM生成正确答案y的置信度）。
- 训练一个多任务重排序器，基于**RoBERTa-large**，训练数据先区分query的难度，得到打分数据

### 1.14 RAG+强化学习

**ReSearch** https://arxiv.org/pdf/2503.19470 

⚠论文涉嫌数据造假，未中稿，复现结果与实验图表差距较大。且实验任务较古老

当前大语言模型（LLMs）虽在推理任务（如 OpenAI-o1、DeepSeek-R1）和检索增强生成（RAG）上表现突出，但仍面临关键挑战：复杂多跳问题需多轮检索与推理结合，而现有多步 RAG 依赖人工设计提示或启发式方法，不仅耗时费力、可扩展性差，且标注推理步骤成本极高。此外，现有强化学习（RL）方法多聚焦提升模型内部推理能力，缺乏与外部知识检索的有效融合。

ReSearch 是一种通过强化学习训练 LLMs 实现 “推理与搜索融合” 的框架，核心创新在于将搜索操作作为推理链的有机组成部分，无需任何推理步骤的监督数据。

推理链包含三类核心元素，通过标签明确区分，实现 “思考 - 搜索 - 结果利用” 的迭代交互：

- **文本思考**：用包裹，指导 “何时搜索” 与 “搜索什么”；
- **搜索查询**：用`<search> </search>`包裹，由文本思考生成，用于检索外部信息；
- **检索结果**：用`<result> </result>`包裹，由搜索工具返回，反哺后续文本思考。

强化学习的方案：

- **算法选择**：采用**分组相对策略优化（GRPO）**，通过一组滚动轨迹（rollout）估计基线，无需单独训练 Critic 模型，同时加入 KL 散度惩罚，避免模型偏离原始参考策略。

- **滚动轨迹生成**：生成过程中遇到`</search>`时，触发搜索工具获取结果并以`<result>`标签插入，继续生成直至遇到结束符（eos）；计算损失时屏蔽检索结果 tokens，避免模型偏向检索内容。

- 奖励机制：仅依赖简单规则化奖励，无需监督数据，包含两部分：

  - **答案奖励**：通过 F1 分数衡量预测答案（`\boxed{}`内内容）与真实答案的一致性；
- **格式奖励**：检查滚动轨迹是否符合标签规范（如`<search>``<result>`正确使用、`\boxed{}`存在），具体规则如下：

**核心结论**

- ReSearch 无需推理步骤监督数据，通过强化学习实现推理与搜索的深度融合，显著提升多跳问答性能；
- 框架具备强泛化性，单一数据集训练（MuSiQue）可适配多种多跳任务（HotpotQA，2Wiki等）；
- 训练过程中自然涌现反思、自我纠错等高级推理能力，无需预定义启发式规则。

### 1.15 召回策略

https://arxiv.org/pdf/2509.04820

https://mp.weixin.qq.com/s/ahK2XkDp9WsmocuBA6OnDg 或许是可落地的

Fishing for Answers: Exploring One-shot vs. Iterative Retrieval Strategies for RAG

问题：行业闭源文档，48%的错误源于top-k召回时遗漏关键片段，导致回答不完整/不准确。如何覆盖分散的关键信息？

两个策略：

1. 在token预算内尽可能多选择相关片段，以 “相关性 /token 比” 最大化证据密度，目标函数为最大化总相关性且 token 总和不超过预算。
2. 然后再基于片段元信息（关键词、时间等）过滤无关片段，再补充实体相关片段，再利用LLM对现有片段的冗余进行裁剪和压缩，保留关键信息。核心策略是权衡，相关性的chunk占token过多，也不能贪多选很多低相关片段。
3. 使用agentic RAG多轮动态检索。

成本可控，简单补丁，贴近落地。

### 1.16 ToG系列

#### ToG1与2阅读

总结对比：



#### 对比基线：多轮RAG专题

RAT(https://arxiv.org/pdf/2403.05313)、
Interactive-KBQA（https://arxiv.org/abs/2402.15131）、
FLARE （https://aclanthology.org/2023.emnlp-main.495.pdf）

RAG在上下文管理上面临着问题，而以上论文诞生于多轮RAG即迭代式（iterative）阶段，特点鲜明。

| 维度             | FLARE（长文本生成）                              | Interactive-KBQA（知识图谱问答）                     | RAT（长周期推理）                               |
| :--------------- | :----------------------------------------------- | :--------------------------------------------------- | :---------------------------------------------- |
| **核心痛点**     | 单轮检索无法覆盖长文本生成的动态信息需求         | 复杂问题难以单轮解析为结构化逻辑形式                 | 思维链易出现事实幻觉且缺乏外部修正机制          |
| **核心技术**     | 前瞻式主动检索（临时句生成→置信度判断→检索修正） | 智能体工具交互（3 类 KB 工具 + 多轮 Thought-Action） | 检索增强 CoT 修订（分步推理→逐段检索→迭代修正） |
| **检索触发逻辑** | 低置信度 token（概率＜阈值 θ）触发               | 推理步骤拆解需求触发（如需定位实体 / 关系）          | 思维链单步推理完成后强制触发                    |
| **关键设计亮点** | 前瞻式查询（对齐未来生成意图）                   | 跨 KB 通用工具 + 人工干预修正                        | 因果式修订（仅依赖已修正历史步骤）              |
| **典型实验提升** | 2WikiMultihopQA F1 提升 8%-10%                   | CWQ 数据集 F1 从 36.5% 升至 49.07%                   | HumanEval pass@1 提升 17%-21%                   |

2025 年主流的**Agentic RAG**（智能体驱动的自主检索规划）与**Context Engineering**（上下文的系统化设计与优化），已从 “检索策略自主性”“上下文管理精细化”“系统架构扩展性” 三个维度实现技术突破，三篇早期工作的局限性主要体现在以下方面：

##### 1. 检索决策机制：从 “被动触发” 到 “自主规划” 的代差（Agentic RAG 视角）

三篇工作的检索决策均依赖**预定义规则**，缺乏 Agentic RAG 的 “自主判断与规划能力”，具体表现为：

- **FLARE 的触发逻辑僵化**：仅通过 “token 置信度阈值” 单一指标决定是否检索，无法像 Agentic RAG 那样结合 “任务类型（如摘要 / 问答）、知识新鲜度（如实时数据需求）、历史检索质量” 做综合决策。例如生成 2025 年美国大选摘要时，FLARE 可能因 “候选人最新民调” token 置信度未低于阈值而漏检，而 Agentic RAG 会主动判断 “时效性需求” 并触发检索。
- **Interactive-KBQA 的工具选择固化**：强制使用预定义的 3 类 KB 工具，并局限于静态KG
- **RAT 的检索流程机械**：对每个思维步骤强制检索，缺乏 Agentic RAG 的 “检索结果评估与重检机制”。例如检索到错误的 Minecraft 合成配方时，RAT 会直接用错误知识修正 CoT，而 Agentic RAG 会评估 “检索结果与任务的相关性”，自动发起二次检索直至获取准确信息。

##### 2. 上下文管理：从 “简单拼接” 到 “系统化工程” 的不足（Context Engineering 视角）

三篇工作均将上下文视为 “检索结果的直接拼接”，未践行 Context Engineering 的 “优化、压缩、结构化” 理念，导致上下文效率与质量不足：

- **上下文冗余与噪声**：FLARE 将检索到的文档全文嵌入上下文，未做 “相关性筛选与摘要压缩”；RAT 直接拼接多轮检索结果，易超出 LLM 上下文窗口。而现代 Context Engineering 会通过 “智能去重、关键信息提取、分块锚点定位” 优化上下文，例如仅保留 “原木合成木板” 的核心步骤，而非整页 Wiki 内容。
- **多模态与多源信息适配缺失**：三篇工作均局限于 “文本类检索结果”，无法像 Context Engineering 那样整合 “表格、图像、数据库数据” 等多模态信息。例如回答 “某产品参数对比” 时，Interactive-KBQA 无法检索并结构化表格数据，只能依赖文本描述，而现代系统可通过多模态处理器将表格转为结构化上下文。
- **上下文与指令的协同不足**：三篇工作仅将检索结果作为 “补充知识”，未像 Context Engineering 那样将 “推理思维模板、输出格式要求、用户角色信息” 与知识融合为结构化提示。例如生成专业法律摘要时，无法像现代系统那样在上下文中嵌入 “法律术语规范、判决案例引用格式” 等指令，导致输出质量依赖模型自身能力。

##### 3. 系统架构：从 “单模块耦合” 到 “多智能体协同” 的局限

三篇工作均为 “单 LLM + 单检索模块” 的耦合架构，无法适配 Agentic RAG 的 “多智能体协作” 需求：

- **缺乏专业化分工**：例如处理 “跨领域复杂问题”（如 “分析某药企新药的专利状态与临床数据”）时，RAT 需独自完成 “问题拆解、专利检索、临床数据检索、推理修正” 全流程，而 Agentic RAG 可分配 “专利智能体、医疗数据智能体、推理智能体” 协同工作，效率与准确性显著提升。
- **无持久化记忆机制**：三篇工作均为 “无状态系统”，无法像 Agentic RAG 那样通过 “短期对话记忆 + 长期知识记忆” 复用历史信息。例如多轮对话中重复询问 “CBS 持股相关问题” 时，Interactive-KBQA 会重新执行完整检索流程，而 Agentic RAG 可直接调用记忆中的历史检索结果，降低冗余开销。

##### 4. 技术定位：从 “核心方案” 到 “组件化工具” 的角色转变

随着**长上下文模型**（如 GPT-4o、Claude 3.5，窗口达 1M tokens）的普及，三篇工作的核心场景已被部分替代：

- **FLARE 的长文本生成场景弱化**：长上下文模型可直接读取全文生成摘要，无需 FLARE 的 “分段生成 + 逐句检索” 流程，仅在小模型（如 Mistral-7B）场景仍有价值。
- **RAT 的 CoT 修正需求降低**：大模型自身的思维链能力已显著提升，结合长上下文可直接生成更准确的 CoT，RAT 的 “分步检索修正” 仅在低参模型或超复杂推理（如数学定理证明）中必要。

##### 总结：过时性的本质与价值留存

三篇工作的 “过时性” 并非技术失效，而是**技术定位从 “前沿方案” 退化为 “基础组件”**：

- 其 “多轮动态检索” 的核心思想仍是 Agentic RAG 的基础，但检索决策、上下文管理等环节需与 “自主规划、系统化工程、多智能体协作” 深度融合；
- 在**小模型落地、静态知识库问答、低资源场景**等领域，三者的设计仍具实用价值 —— 例如 FLARE 的置信度触发逻辑可作为 Agentic RAG 的 “轻量级检索模块”，Interactive-KBQA 的人工干预机制可用于构建高质量 Agent 训练数据。



### 1.17 刘焕勇 CCKS报告

https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648423217&idx=1&sn=3205a735b04fa23b9c94780682a008a4&chksm=8266620c383f3f2953e8eb80202a0906b99e2fd8480113432f1b028cb7705cd286029665cf3e&scene=0&xtrack=1&subscene=90#rd

**《面向“搜、问、推、写”应用场景的文档解析及知识库建设实践》**

总结为关于知识库及RAG落地的十条建议，是个方法论和方向的指引：

1. 不要为了上RAG而上RAG，尤其是NL2SQL,KBQA这种类型，之前解决的很好的就不要再折腾了。
2. 不要为了上变体而上变体，GraphRAG、多模态RAG、DeepResearch等能不上就不上，把最基本RAG做出来就好。
3. 通用的RAG是一种标品，标品从来都解决不了优化问题，要放弃这种思路。
4. RAG本身就是破布，是面向具体业务问题而做的补丁，要有这种意识，面向业务做RAG，而不是面向RAG做业务，具体case 具体分析，评估先行，可用的RAG一定是有很多路由逻辑的。
5. 目前开源的RAG框架有很多，其意义其实并不是为了生产，而是为了快速做场景验证，要做开源框架祛魅
6. 能自己动手写就动手写，RAG没多少复杂的东西，开源框架同质化，黑盒化，不利于做问题定位，要适当抛弃；
7. RAG本身就是无处不在的，它是一种框架，而不是一种单独的技术，更多时候还是一种工程架构
8. 决定RAG好不好用的，不是RAG技术本身，而在于用户的问题域是否建模清楚，以及业务实现逻辑的设计。
9. 落地总是二八原则，很多优化方案都是解决20%长尾问题而设计，这个需要清楚，需要衡量ROI投入产出比；
10. RAG的文档解析要做，但并不需要文档解析做到100%还原，这是一条歧路。应该投入，但不要过度关注。文档解析是手段，不是目的。

另外：

**1、GraphRAG中的“KG”不是“KG”**

GraphRAG里抽取图谱的目的是做索引，做锚点，做关联，能连上就行，所以，不要跟做专门抽取的混为一谈，也可以不抽实体和关系。GraphRAG还是专注在做问答任务，说白了还是RAG，不是ie任务，它背后还是用llm做的抽取、索引和关联。

为什么graphrag通常做抽取不带schema?，定不出来（业务难），定出来太少（太少bridge的作用不够），定出来太大（抽取难度大，上下文爆炸），也就是没也不好，太少也不好，太多也不好，很主观。

**2、GraphRAG的跳转作用可以用于复杂推理数据合成**

复杂推理任务，核心特征是需要多跳，Graph中的关联关系天然的给出了数据输入。目前，这块是提升大模型推理能力的一个重要方向。

**DeepDive**(https://arxiv.org/pdf/2509.10446)，通过开放知识图谱自动合成含“模糊实体”的复杂问题，构建深度搜索问答数据集。

**MedResearcher-R1**(https://arxiv.org/abs/2508.14880)，通过医学知识图谱生成复杂的多跳问答

2023年大模型落地，有三驾马车【文档解析+知识图谱+大模型】

2025年变成新的三驾马车【知识图谱+强化学习+大模型】

三者的关系是：**通过知识图谱来增强LLM的RL能力，使之能更好地利用知识图谱本身，解决领域知识推理问题**。

