import{_ as e,c as a,a as r,o as n}from"./app-Bpj5Mkzv.js";const i={};function l(p,t){return n(),a("div",null,t[0]||(t[0]=[r('<h1 id="rag方法与思路总结" tabindex="-1"><a class="header-anchor" href="#rag方法与思路总结"><span>RAG方法与思路总结</span></a></h1><h2 id="基本流程回顾" tabindex="-1"><a class="header-anchor" href="#基本流程回顾"><span>基本流程回顾</span></a></h2><p>知识库-向量化（文本切块）</p><p>查询-向量化-召回（查询重写，关键词和语义的混合检索，结构化知识表示与召回，召回结果的重排序）</p><p>上下文学习-生成（预设提示词，temperature）</p><p>🎉<strong>常看常新：2025年CCF-A主会RAG论文</strong></p><p>https://www.notion.so/RAG-2025-2270b854c75b808fb90dec88c4ed1140</p><h2 id="_1-关键点记录" tabindex="-1"><a class="header-anchor" href="#_1-关键点记录"><span>1 关键点记录</span></a></h2><h3 id="_1-1-代码code的rag" tabindex="-1"><a class="header-anchor" href="#_1-1-代码code的rag"><span>1.1 代码code的RAG</span></a></h3><p>常见的切分方法：按照函数块切分，按照内部的逻辑块例如循环切分，混合切分并使用部分重叠。</p><p>以上的切分方法会导致无关内容的合并，以及现有语义的断裂。</p><p>改进的方法：借助抽象语法树AST，对代码分块，保留语法结构，递归分块，并在大小限制下合并兄弟节点</p><p>https://github.com/yilinjz/astchunk</p><p>另一个项目：基于图的代码RAG https://github.com/vitali87/code-graph-rag</p><p>能解决的问题：检索代码中符合要求的片段，可能用于帮助理解一个项目。</p><p><strong>对比：cursor对于项目代码的理解</strong></p><p>结构化代码解析（AST、符号表、依赖包关系分析）+文档切分策略优化+向量化与数据库存储+混合检索重排序与生成。本质上是对代码分块，根据问题召回代码块辅助生成。</p><p>cursor如何回答关于项目结构的问题？结构化表示项目目录，搜索可能存在的readme文档</p><h4 id="cursor的代码库索引" tabindex="-1"><a class="header-anchor" href="#cursor的代码库索引"><span>Cursor的代码库索引</span></a></h4><p>https://mp.weixin.qq.com/s/QAV5dTNBbBqeUfMP6qAN5A</p><p>https://read.engineerscodex.com/p/how-cursor-indexes-codebases-fast，主要思想还是利用Merkle树快速索引代码库。Merkle树是一种树形结构，其中每个“叶”节点都标记有数据块的加密哈希值，每个非叶节点都标记有其子节点标签的加密哈希值，这种结构创建了一个分层结构。</p><p>Cursor首先在本地将代码库文件分割成语义上有意义的块，启用代码库索引时，Cursor会扫描编辑器中打开的文件夹，并计算所有有效文件的哈希值的Merkle树。代码块被发送到Cursor的服务器后，会使用OpenAI的嵌入API或自定义嵌入模型生成嵌入。但是，还是老问题，代码库索引的有效性在很大程度上取决于代码是如何被分割的。</p><p>在工具上，可以使用像tree-sitter这样的工具来进行AST解析，它支持多种编程语言。</p><p>《<strong><a href="https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;mid=2648421306&amp;idx=1&amp;sn=ac93f33cc8e1aa6a9b1722c2534b2554&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">代码RAG第二弹：代码类的GraphRAG怎么做？一个示例项目</a></strong>》</p><h3 id="_1-2-多模态graphrag" tabindex="-1"><a class="header-anchor" href="#_1-2-多模态graphrag"><span>1.2 多模态GraphRAG</span></a></h3><p>https://mp.weixin.qq.com/s/UtSjX_D2k-Cp7iJT-CNqtQ</p><p>创新点：多模态大模型用于数据合成和数据增强（对现有数据集的裁剪、旋转等）</p><p>以文档为中心的多模态GraphRAG，依赖于底层知识库MultimodalDocGraph。文档场景下的问题：引用“见表1”需要链接当前文本块和图表，可以采用知识图谱的实体链接。因此，需要将知识图谱拓展到多模态元素。实现过程中，图片，表格，公式都可以被进一步文本化。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/fUBU1yiaEmJj1lNlicSZMWmdunBBGyCt7ayTf8HIFFFbMLjSISibGxY9daGpdWKg95libNI5adnrMCCVibmkc2gBbPQ/640?wx_fmt=png&amp;from=appmsg&amp;watermark=1&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1#imgIndex=1" alt="img"></p><p>开源项目实现 https://github.com/HKUDS/RAG-Anything</p><p>代码实现效果：</p><ul><li>文档处理方式不合理，速度慢：所有的类型的文档都可以接受，统一转成IMG和PDF（即使是纯文本），并借助MinerU进行处理。图像、表格、方程分别用不同的方法解析，图像统一base64编码。</li><li>实体与图像实体存在一对多关系，容易造成较大的噪声；</li><li>框架较大，难评用处大小</li></ul><p>图谱构建过程：</p><ol><li>chunk切分</li><li>文本chunk使用命名实体识别、关系抽取等提取关键信息，构造**&lt;entity, rel, entity&gt;**</li><li>图像等作为模态实体（modal_entity），借助视觉模型分析文本与图像的关系，建立连接 <ol><li><strong><code>&lt;entity, belongs_to, modal_entity&gt;</code></strong> 文本与模态实体的关联</li><li><strong><code>&lt;chunk, related_to, modal_entity&gt;</code></strong> chunk与模态实体</li><li><strong><code>&lt;entity, locate_in, chunk&gt;</code></strong> 实体与chunk</li></ol></li><li>最终图谱包括三类节点，以及以上四个主要关系边</li></ol><p><strong>文档为中心的多模态GraphRAG及MultimodalDocGraph是个很好的故事，这个可以讲一个比较好的故事，多模态、kg、文档解析、rag都通了。😊</strong></p><p>类似的论文（河南大学，未开源）</p><p>https://arxiv.org/pdf/2509.10467</p><p>设计了一个复杂的工作流，处理文档的逻辑结构（directory），一级多模态元素（文本/图像/表格）</p><h4 id="构建图的多种思路" tabindex="-1"><a class="header-anchor" href="#构建图的多种思路"><span>构建图的多种思路</span></a></h4><p>可以从实体触发，可以从query出发，也可以从chunk出发。</p><p>《<strong>Query-Centric Graph Retrieval Augmented Generation</strong>》(https://arxiv.org/pdf/2509.21237) 华为团队</p><p>核心的思路是<strong>Doc2Query</strong>生成基于文本块的查询-答案对，然后<strong>构建查询中心图</strong>（QCG），最后采用<strong>多跳检索机制</strong>从图中筛选相关文本块，最终送LLM做生成。</p><p>问题在于应用场景十分受限，每个chunk都生成QA对，计算量和索引量都非常大。</p><h3 id="_1-3-deep-research进展" tabindex="-1"><a class="header-anchor" href="#_1-3-deep-research进展"><span>1.3 Deep Research进展</span></a></h3><p>商业与非商业的实现的总结 https://arxiv.org/pdf/2506.12594</p><p>现有的开源agent框架 https://github.com/scienceaix/deepresearch</p><p>涉及到的组成部分：</p><ul><li>基座与推理模型（上下文管理，记忆，CoT，ToT）</li><li>任务规划与执行（任务分解，层级规划，自动执行与监控，多agent协同）</li><li>工具使用与环境交互（网页导航与交互，内容处理，API调用，领域特定tool使用）</li><li>知识生成（结果评估，来源认证，生成结构化报告，交互）</li></ul><p>实现架构：</p><ol><li>单体式，以核心推理引擎集中控制</li><li>流水线式，定义严格的工作流和各阶段之间的接口与数据格式</li><li>多智能体式，设计明确的通信协议，进行不同角色和责任的多智能体协作</li><li>混合模式</li></ol><p>评估要点：基础模型，推理效率，上下文长度，工具集成与环境适应性，任务规划与执行稳定性，知识聚合与输出质量，不同场景的适应性（学术研究，企业决策，个人知识管理）</p><p>待解决的问题：隐私，知识产权，可访问性</p><p>未来研究方向：高级推理架构，多模态集成，领域特化，基于人机协作的标准化</p><h3 id="_1-4-embedding模型进展" tabindex="-1"><a class="header-anchor" href="#_1-4-embedding模型进展"><span>1.4 Embedding模型进展</span></a></h3><p>https://mp.weixin.qq.com/s/HYd2EkU01O3IgQn2ENaEkw</p><p>传统embedding模型固定向量维度输出，参数量小。</p><p>从Nvidia的NV-Embed到jina-v4，embedding模型主逐渐转向：</p><ul><li>单模态-多模态</li><li>单一向量-多向量</li><li>固定维度-自定义维度</li><li>小模型-大模型</li></ul><p>并且，开始使用合成数据等方式挖掘难样本，并针对特定任务补充能力项。</p><p><strong>对比Qwen3系列embedding模型和reranker模型：</strong></p><table><thead><tr><th></th><th>Embedding</th><th>Reranker</th></tr></thead><tbody><tr><td>处理数据类型</td><td>单个文本转化为语义向量</td><td>文本对（查询和文档），输出相关性分数</td></tr><tr><td>模型架构</td><td>双编码器架构</td><td>交叉编码器，对query和文档联合编码（极慢）</td></tr><tr><td>技术基础</td><td>基于大语言模型</td><td>基于大语言模型</td></tr></tbody></table><p>训练过程：构造数据-lora微调-模型合并</p><p>RAG流程：</p><ul><li>预处理：知识库切块，利用embedding模型向量化</li><li>RAG：问题向量化，余弦相似度匹配初步召回，重排序top-n，合并查询上下文构造prompt，生成</li></ul><h4 id="一个趋势-小参数的embedding模型" tabindex="-1"><a class="header-anchor" href="#一个趋势-小参数的embedding模型"><span>一个趋势：小参数的embedding模型</span></a></h4><p>Google的Embedding Gemma-308M模型，支持100+语言，输出嵌入维度大小可选。</p><p>3.08亿个参数，1亿模型参数，2亿嵌入参数。</p><ul><li>嵌入参数指的是“词表大小V×嵌入维度D” <ul><li>嵌入维度指的是输入嵌入的维度，即初始文本的每个token初步处理成高维向量时的维度</li></ul></li></ul><p>⚠<strong>目前尺寸更长、维度可定义、模型更小、数据更多样化、特定任务特定instruction的趋势逐步成为标配</strong>。</p><p><strong>使用案例</strong>：</p><ol><li>文本</li><li>分词token</li><li>初始嵌入</li><li>transformer捕捉上下文关系生成深层表示</li><li>对token级向量均值池化（特征压缩），聚合成单一的上下文向量。原始每个token都有一个1×768维的向量表示，通过按句子压缩，每一个token的同一维度被聚合，得到对于句子的1×768维向量。好处：高校、防止过拟合，无论句子有多少token向量表示维度都一致。</li><li>通过全连接层生成指定的输出维度</li></ol><p>小模型应用场景：高响应速度需求、端侧部署</p><h3 id="_1-5-文档智能专题" tabindex="-1"><a class="header-anchor" href="#_1-5-文档智能专题"><span>1.5 文档智能专题</span></a></h3><p>https://mp.weixin.qq.com/s/QAV5dTNBbBqeUfMP6qAN5A</p><p>文档解析可以使用单独的多模态大模型进行解析，也可以构建工作流，进行布局分析，文本提取。继续做差异性可以在于：跨页图表/段落的合并。跨页文档，甚至有专门的数据集由于评测。需要判断以下问题：</p><ul><li>文档问题回答失败的核心，是否是跨页导致的</li><li>为了解决跨页问题导致的时间开销是否有必要</li></ul><p>过去的典型项目：https://mp.weixin.qq.com/s/5852Kn8wVlsSGpclrjkBNA，涉及RAGflow（企业级领域知识库RAG问答工作流搭建解决方案），MinerU（专长于PDF分析）等， 提供了很多细节的文档解析功能。</p><h4 id="数据构建" tabindex="-1"><a class="header-anchor" href="#数据构建"><span>数据构建</span></a></h4><p>文档解析模型，布局检测、多模态解析模型，都可以用到大模型训练语料的处理当中。</p><p>https://mp.weixin.qq.com/s/idPm-boNEsZtMaNOXJJe9A</p><p>https://huggingface.co/datasets/HuggingFaceFW/finepdfs</p><p>构造过程中集成了OCR，布局检测，语言识别，hash去重，姓名隐私化处理等，还训练了单独的xgboost模型用于需求检测与分类。</p><h4 id="面向文档布局优化的多模态文档" tabindex="-1"><a class="header-anchor" href="#面向文档布局优化的多模态文档"><span>面向文档布局优化的多模态文档</span></a></h4><p>《<strong>Logics-Parsing: An End-to-end Document Parsing Model with Layout-centric Reinforcement Learning</strong>》，https://github.com/alibaba/Logics-Parsing，https://arxiv.org/pdf/2509.19760</p><p>在解决复杂文档问答问题时，对于报纸、poster等多列多图的多模态资源，布局分析与阅读顺序是优化的一个方向。</p><p>采用：</p><p>面向多任务的SFT（布局分析、内容提取、分类、逻辑解析）</p><p>+面向布局的RL（文字提取效果、布局分析准确率、阅读顺序）</p><h4 id="多模态长文档rag" tabindex="-1"><a class="header-anchor" href="#多模态长文档rag"><span>多模态长文档RAG</span></a></h4><p>https://mp.weixin.qq.com/s/O6qWnWJ9HnfaYDTM3DKGUQ</p><h5 id="检索技术" tabindex="-1"><a class="header-anchor" href="#检索技术"><span>检索技术</span></a></h5><p>《A Survey of Long-Document Retrieval in the PLM and LLM Era》，https://arxiv.org/pdf/2509.07759 苏州大学</p><h5 id="benchmark" tabindex="-1"><a class="header-anchor" href="#benchmark"><span>benchmark</span></a></h5><p>《<strong>VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding</strong>》，https://arxiv.org/pdf/2508.07493，https://github.com/puar-playground/VisR-Bench</p><p>建模真实场景中多语言、多页的长文档检索任务，整理了常见的相关数据集与基线模型</p><h3 id="_1-6-多模态rag" tabindex="-1"><a class="header-anchor" href="#_1-6-多模态rag"><span>1.6 多模态RAG</span></a></h3><p>https://mp.weixin.qq.com/s/BbT0XCGbjwJ6mXb2EuVyGg</p><p>做多模态RAG需要有多模态Embedding模型，例如ColBERT（本身是文本embed），ColPali。</p><p>以上的传统解决方案存在局限性：</p><ul><li>检索结果是页面级的，没法分析页面的具体内容</li><li>ColBERT依赖于文本信息，对文本中的数值信息解析能力差</li><li>框架M3DocRAG，能够结合文本和图像信息，但缺乏信息的细致提取，和跨模态整合能力</li></ul><p>因此，尝试使用agent解决，https://arxiv.org/pdf/2503.13964，https://github.com/aiming-lab/MDocAgent，多模态多智能体框架，利用文本和图像信息来提高文档问答的准确性。</p><p><strong>MDocAgent</strong>：</p><ul><li>设计五个由prompt驱动的agent（初步生成，提取关键信息，文本处理，图像处理，综合智能体整合），<strong>ColBERTv2和ColPali</strong>分别作为文本和图像检索器</li><li>文档智能方面，还是使用OCR+PDF解析提取文本，按页保存为图像，生成文本和视觉表示</li><li>实现过程中，分别用两个检索器进行检索，agent各自分工处理文本和多模态，最终整合</li></ul><h4 id="评估数据集" tabindex="-1"><a class="header-anchor" href="#评估数据集"><span>评估数据集</span></a></h4><h5 id="double-bench" tabindex="-1"><a class="header-anchor" href="#double-bench"><span>Double-Bench</span></a></h5><p>https://arxiv.org/pdf/2508.03644</p><p>多模态文档检索数据集，单跳/多跳，多语言，多类文档</p><p>数据构建过程是亮点：：原始文档经过粗粒过滤（10-50页，使用GPT-4o进行语言验证），然后使用Docling和MinerU等工具进行模态分解，将每页拆分为构成文本、表格和图形组件，然后生成单跳查询，同时额外构建知识图谱，以辅助多跳查询的生成。这个点值得借鉴，<strong>做多跳数据生成，还是使用知识图谱来做中间辅助。</strong></p><p>很棒的一篇多模态数据集工作：</p><ul><li>embedding模型选型（文本+多模态）</li><li>MLLM选型</li><li>评测了文档多模态RAG框架（<strong>MDocAgent</strong>、<strong>ViDoRAG</strong>、<strong>M3DOCRAG</strong>、<strong>Colqwen-gen</strong>）</li></ul><p>⚠<strong>框架的回答准确性与检索准确性高度相关</strong>，Colqwen-gen（强检索+简单生成）的多跳回答正确率接近复杂框架MDocAgent，证明“优化检索阶段”比“设计复杂生成流程”更关键；</p><p><strong>主流框架（如MDocAgent、ViDoRAG）倾向“有问必答”</strong>，即便未检索到证据，仍生成推测性内容，过度自信。复杂框架（MDocAgent、ViDoRAG）因多智能体串行协调，推理时间是Colqwen-gen的4倍左右。</p><h4 id="一个小专题-非标准印刷体-问题图像的文档解析" tabindex="-1"><a class="header-anchor" href="#一个小专题-非标准印刷体-问题图像的文档解析"><span>一个小专题：非标准印刷体/问题图像的文档解析</span></a></h4><p>几何弯曲、阴影、污渍等变化直接影响layout与ocr的效果</p><p>前沿的方案可以看《<strong>DocRes: A Generalist Model Toward Unifying Document Image Restoration Task</strong>s》，代码在：https://github.com/ZZZHANG-jx/DocRes/tree/master，https://arxiv.org/pdf/2405.04408</p><p>其意义在于提出一个统一了<strong>五种文档图像还原任务的通用模型，包括去扭曲、去阴影、外观增强、去模糊和二值化</strong>。<u>方法上，采用自行构建的一个模型。</u></p><p>其他图像增强的文档处理整合</p><p>https://github.com/ZZZHANG-jx/Recommendations-Document-Image-Processing</p><h3 id="_1-7-prompt工程" tabindex="-1"><a class="header-anchor" href="#_1-7-prompt工程"><span>1.7 prompt工程</span></a></h3><p>RAG落地过程中prompt也是很重要的一环。</p><p>https://github.com/asgeirtj/system_prompts_leaks/ 各大主流大模型的系统提示词</p><p>提示词的历史发展：</p><ul><li>zero-shot-CoT： Let&#39;s think step by step</li><li>optimizers： Take a deep breath and work on this problem step-by-step</li><li>emotion prompt： Are you sure？ This is very important to me. <ul><li>PUA类：你确定？相信你的能力。你要将这个困难挑战视为成长机会</li></ul></li></ul><h4 id="腾讯混元-promptenhancer" tabindex="-1"><a class="header-anchor" href="#腾讯混元-promptenhancer"><span>腾讯混元 PromptEnhancer</span></a></h4><p>https://www.arxiv.org/pdf/2509.04545</p><p>针对性的训练一个文生图prompt改写的模型</p><ol><li>模型蒸馏：构造高质量训练集图片，先生成简短caption，再借助Gemini生成CoT和多个候选prompt结果</li><li>训练阶段，先做SFT，再做GRPO强化</li></ol><p>评估不好做，拉维度来凑，所以设计了一个<strong>包含24个细粒度关键点的分类体系，关键点被组织成六个主要类别</strong>，涵盖包括：<strong>语言和语法理解</strong>，评估对否定等核心语言结构的解释；<strong>视觉属性</strong>，关注对象数量等渲染属性；<strong>动作和互动</strong>，衡量动态状态的表现；关系和组合结构，评估复杂场景的处理；以及由<strong>知识和想象力以及图像内文本和布局所涵盖的更高层次能力</strong>。</p><p>总结：<strong>无论是做改写，还是做生成，都是需要有反馈，有reward，先sft，后grpo做强化，是目前常用策略</strong>。</p><h3 id="_1-8-memory上下文管理" tabindex="-1"><a class="header-anchor" href="#_1-8-memory上下文管理"><span>1.8 Memory上下文管理</span></a></h3><p>对于agent，常见的上下文管理包括：</p><ul><li><p>短期记忆：系统提示词+顺序存储完整上下文+用户问题</p></li><li><p>长期记忆：控制聊天记录长度做为短期内存，单独存储长期内存进行管理并检索相关记忆（效率会变低，但可以进行用户定制，跨越多个对话和历史对话）</p></li><li><p>update类：针对memory管理使用curd操作。没出现过的加入，出现过有矛盾的要删除旧的，同一方向有补充的要更新，本质上是基于语义做RAG。如果还不够，在做知识图谱的实体与关系抽取。例子参考mem0：http://arxiv.org/pdf/2504.19413</p></li></ul><p>记忆管理的榜单：https://github.com/NevaMind-AI/memU memu方法准确度最高，但单词检索需要1s，更不用说需要把检索结果用于大模型再生成内容。</p><h3 id="_1-9-rag-科研-ultrarag-flashrag" tabindex="-1"><a class="header-anchor" href="#_1-9-rag-科研-ultrarag-flashrag"><span>1.9 RAG+科研：UltraRAG / FlashRAG</span></a></h3><p>https://mp.weixin.qq.com/s/LEtuEKtZyiUqd1pdHW1XBA</p><p>https://github.com/OpenBMB/UltraRAG</p><p>定位：基于MCP架构设计的 “面向科研的RAG实验加速器”</p><p>使用方法：利用现有的组件，编写yaml配置文件，实现串行并行，分支循环，内置常用数据集</p><p>https://github.com/RUC-NLPIR/FlashRAG</p><p>定位：快速复现现有RAG框架</p><p>使用方法：可视化界面操作</p><h3 id="_1-10-graphrag的应用与落地" tabindex="-1"><a class="header-anchor" href="#_1-10-graphrag的应用与落地"><span>1.10 GraphRAG的应用与落地</span></a></h3><p>集成式RAG框架，https://github.com/apecloud/ApeRAG/ 在lightRAG（实体抽取、关系抽取、实体合并）基础上，使用了向量搜索和全文关键词搜索，并且以MinerU辅助进行文档解析。整体上是工程的优化，例如实现了知识图谱的隔离，可以做多租户。</p><p>GraphRAG落地的工程难题：</p><ul><li>实体关系抽取时需要用LLM对文本块逐个调用大模型，而且初步抽取后可能还需要动态判断是否需要补充</li><li>知识图谱的隔离管理</li><li>实体合并与去重的准确性挑战，需要强大的上下文理解能力，尤其在专业领域内不能出错</li><li>依赖项较多：MinerU，LLM，图数据库，向量数据库，全文搜索引擎</li></ul><h4 id="专题-知识抽取时的schema" tabindex="-1"><a class="header-anchor" href="#专题-知识抽取时的schema"><span>专题：知识抽取时的schema</span></a></h4><p>Schema 指的是对图中实体（Entity）、关系（Relation）和属性（Attribute）的预先定义的结构化模式或本体（Ontology）。抽取前，事先明确定义好允许的实体类别（如 Person, Organization, Product）和关系类型（如 works_for, located_in, develops），抽取过程必须遵循这个预设框架。</p><p>因此，schema是一个工程化的解决方案。如果由专家预设的schema，覆盖足够全，则效果很好。但需要专家人工检查的成本，如果漏掉关键信息无法补充。</p><p>为什么主流GraphRAG（如LightRAG）选择“无Schema”？因为graphrag中的kg，其实作用做bridge的作用，也就是做锚点用，所以要尽可能的多，也无需要太多限制。GraphRAG并不追求构建一个完美的、可用于独立问答的知识库，而是服务于<strong>下游检索任务</strong>。只要这些节点能帮助用户从A跳到C（即使B有点噪声），它的目的就达到了。</p><h3 id="_1-11-rag加速" tabindex="-1"><a class="header-anchor" href="#_1-11-rag加速"><span>1.11 RAG加速</span></a></h3><p>Meta的工作，REFRAG（REpresentation For RAG）RAG解码框架，用了一个“压缩-感知-扩展”策略减少冗余计算，为工程策略。</p><h3 id="_1-12-chunk切分策略" tabindex="-1"><a class="header-anchor" href="#_1-12-chunk切分策略"><span>1.12 Chunk切分策略</span></a></h3><p>Is Semantic Chunking Worth the Computational Cost? https://arxiv.org/pdf/2410.13070</p><p>论文通过实验证明了语义分块并不一定比固定分块效果好。</p><p>技术总结：https://weaviate.io/blog/chunking-strategies-for-rag</p><p>主要分为两大类：</p><ul><li>预分块：向量化之前，对原始文档分块，提前决定块的大小和边界。优点在于提前建立好了索引，检索时效率高。</li><li>后分块：对于整个文档进行嵌入，在接收到查询时再对文档分块。通过缓存机制，存储分块结果，因此查询可以有动态性，分块策略具有上下文感知能力，但是效率相对低。</li></ul><p>通用分块策略：</p><ol><li>固定大小：基于token分块，设置max-token，通过重叠overlap缓解完整性</li><li>递归分块：基于现有的自然分隔符分块，如果有的块过长则递归分成更小的</li><li>基于文档结构：按照标题、章节、段落、代码块等</li><li>语义分块：先分句，然后按照嵌入向量的余弦相似度，相邻的句子如果高于相似度阈值则分为一块。</li><li>大模型分块：直接让大模型把文本编程list。需要较长的上下文窗口</li><li>agentic分块：动态路由，根据文章特点选择不同的分块策略</li><li>后分块：先嵌入文档，块的嵌入表示=块内的token的平均值。理想状况下，每个块的表示都代表了整个文档的上下文。</li><li>层次分块：和文档结构很像，但更需要层次的依赖关系，不只依赖文件格式（第一段，第二段），还依赖章节结构（摘要，引言，方法）</li><li>adaptive：根据文档的内容动态调整参数（如chunk大小，overlap大小）。需要专门的模型判断，较少用</li></ol><h3 id="_1-13-rag-信息论" tabindex="-1"><a class="header-anchor" href="#_1-13-rag-信息论"><span>1.13 RAG+信息论</span></a></h3><p>结合信息论计算RAG召回过程中的理论上界（数学基础）。目标：召回的top-k，定这个k，使得总信息量最大</p><p><strong>Chunk相关性最大化的动态TOP-K策略</strong> https://arxiv.org/pdf/2509.04820</p><h4 id="信息增益" tabindex="-1"><a class="header-anchor" href="#信息增益"><span><strong>信息增益</strong></span></a></h4><p>摸到了应用中基于信息论评估的边界，但仍缺少严格的形式化证明。具体的AAAI审稿时一篇关于文档噪声的证明可借鉴</p><p>https://arxiv.org/pdf/2509.12765</p><p>核心思想：相似度高的不一定对于生成结果有用，以信息量训练一个重排序模型</p><ul><li>定义量化指标DIG：量化检索文档对正确答案生成的贡献，通过计算“有无该文档时LLM生成置信度的差值”（结合查询x与文档di时，LLM生成正确答案y的置信度，减去仅基于查询x时，LLM生成正确答案y的置信度）。</li><li>训练一个多任务重排序器，基于<strong>RoBERTa-large</strong>，训练数据先区分query的难度，得到打分数据</li></ul><h3 id="_1-14-rag-强化学习" tabindex="-1"><a class="header-anchor" href="#_1-14-rag-强化学习"><span>1.14 RAG+强化学习</span></a></h3><p><strong>ReSearch</strong> https://arxiv.org/pdf/2503.19470</p><p>⚠论文涉嫌数据造假，未中稿，复现结果与实验图表差距较大。且实验任务较古老</p><p>当前大语言模型（LLMs）虽在推理任务（如 OpenAI-o1、DeepSeek-R1）和检索增强生成（RAG）上表现突出，但仍面临关键挑战：复杂多跳问题需多轮检索与推理结合，而现有多步 RAG 依赖人工设计提示或启发式方法，不仅耗时费力、可扩展性差，且标注推理步骤成本极高。此外，现有强化学习（RL）方法多聚焦提升模型内部推理能力，缺乏与外部知识检索的有效融合。</p><p>ReSearch 是一种通过强化学习训练 LLMs 实现 “推理与搜索融合” 的框架，核心创新在于将搜索操作作为推理链的有机组成部分，无需任何推理步骤的监督数据。</p><p>推理链包含三类核心元素，通过标签明确区分，实现 “思考 - 搜索 - 结果利用” 的迭代交互：</p><ul><li><strong>文本思考</strong>：用包裹，指导 “何时搜索” 与 “搜索什么”；</li><li><strong>搜索查询</strong>：用<code>&lt;search&gt; &lt;/search&gt;</code>包裹，由文本思考生成，用于检索外部信息；</li><li><strong>检索结果</strong>：用<code>&lt;result&gt; &lt;/result&gt;</code>包裹，由搜索工具返回，反哺后续文本思考。</li></ul><p>强化学习的方案：</p><ul><li><p><strong>算法选择</strong>：采用<strong>分组相对策略优化（GRPO）</strong>，通过一组滚动轨迹（rollout）估计基线，无需单独训练 Critic 模型，同时加入 KL 散度惩罚，避免模型偏离原始参考策略。</p></li><li><p><strong>滚动轨迹生成</strong>：生成过程中遇到<code>&lt;/search&gt;</code>时，触发搜索工具获取结果并以<code>&lt;result&gt;</code>标签插入，继续生成直至遇到结束符（eos）；计算损失时屏蔽检索结果 tokens，避免模型偏向检索内容。</p></li><li><p>奖励机制：仅依赖简单规则化奖励，无需监督数据，包含两部分：</p><ul><li><strong>答案奖励</strong>：通过 F1 分数衡量预测答案（<code>\\boxed{}</code>内内容）与真实答案的一致性；</li></ul></li><li><p><strong>格式奖励</strong>：检查滚动轨迹是否符合标签规范（如<code>&lt;search&gt;``&lt;result&gt;</code>正确使用、<code>\\boxed{}</code>存在），具体规则如下：</p></li></ul><p><strong>核心结论</strong></p><ul><li>ReSearch 无需推理步骤监督数据，通过强化学习实现推理与搜索的深度融合，显著提升多跳问答性能；</li><li>框架具备强泛化性，单一数据集训练（MuSiQue）可适配多种多跳任务（HotpotQA，2Wiki等）；</li><li>训练过程中自然涌现反思、自我纠错等高级推理能力，无需预定义启发式规则。</li></ul><h3 id="_1-15-召回策略" tabindex="-1"><a class="header-anchor" href="#_1-15-召回策略"><span>1.15 召回策略</span></a></h3><p>https://arxiv.org/pdf/2509.04820</p><p>https://mp.weixin.qq.com/s/ahK2XkDp9WsmocuBA6OnDg 或许是可落地的</p><p>Fishing for Answers: Exploring One-shot vs. Iterative Retrieval Strategies for RAG</p><p>问题：行业闭源文档，48%的错误源于top-k召回时遗漏关键片段，导致回答不完整/不准确。如何覆盖分散的关键信息？</p><p>两个策略：</p><ol><li>在token预算内尽可能多选择相关片段，以 “相关性 /token 比” 最大化证据密度，目标函数为最大化总相关性且 token 总和不超过预算。</li><li>然后再基于片段元信息（关键词、时间等）过滤无关片段，再补充实体相关片段，再利用LLM对现有片段的冗余进行裁剪和压缩，保留关键信息。核心策略是权衡，相关性的chunk占token过多，也不能贪多选很多低相关片段。</li><li>使用agentic RAG多轮动态检索。</li></ol><p>成本可控，简单补丁，贴近落地。</p><h3 id="_1-16-tog系列" tabindex="-1"><a class="header-anchor" href="#_1-16-tog系列"><span>1.16 ToG系列</span></a></h3><h4 id="tog1与2阅读" tabindex="-1"><a class="header-anchor" href="#tog1与2阅读"><span>ToG1与2阅读</span></a></h4><p>ToG是agent智能体范式下的主要论文工作</p><p>总结对比：</p><table><thead><tr><th>对比维度</th><th>Think-on-Graph (ToG, ICLR 2024)</th><th>Think-on-Graph 2.0 (ToG 2.0, ICLR 2025)</th></tr></thead><tbody><tr><td><strong>核心解决问题</strong></td><td>1. LLM 仅依赖内部知识，易产生幻觉、无法处理多跳推理；2. 传统 “LLM⊕KG” 松散耦合，LLM 不参与图推理，依赖 KG 完整性；3. 推理过程无追溯性，无法修正错误</td><td>1. 文本基 RAG 无法捕捉实体结构化关系，KG 基 RAG 缺乏细粒度文本上下文；2. 松散混合 RAG（如 CoK、GraphRAG）仅聚合信息，未优化检索；3. 复杂任务需深度整合结构化与非结构化知识</td></tr><tr><td><strong>现有方案缺陷</strong></td><td>1. LLM-only（CoT/SC）：无外部知识支撑，多跳推理能力弱；2. LLM⊕KG（StructGPT/KB-BINDER）：LLM 仅转查询，KG 缺失关系则失效；3. 无知识追溯与修正机制</td><td>1. 文本 RAG（Vanilla RAG）：依赖语义相似性，忽略实体关联；2. KG RAG（ToG）：缺乏实体文本细节，无法补充 KG 空白；3. 松散混合 RAG：无跨源检索优化，多跳效率低</td></tr><tr><td><strong>核心技术范式</strong></td><td>“LLM⊗KG” 紧耦合范式（LLM 作为 Agent 参与 KG 推理）</td><td>“KG×Text” 紧耦合混合 RAG 范式（KG 引导文本检索，文本优化 KG 剪枝）</td></tr><tr><td><strong>主要实现方法</strong></td><td>1. 三阶段流程：- 初始化：LLM 提取 Top-N 主题实体；- 探索：beam search 迭代 “关系检索 - LLM 剪枝 - 实体检索 - LLM 剪枝”；- 推理：LLM 评估路径充足性，生成答案；2. 变体 ToG-R：以 “关系链” 替代三元组，随机剪枝实体降本</td><td>1. 四阶段流程：- 初始化：实体提取 + 文档片段检索，初判是否直接回答；- 图检索：KG 关系剪枝→候选实体检索；- 文本检索：三元组转语句 + DRM 计算文本相关性，剪枝实体；- 推理：LLM 结合三元组与文本评估，生成线索优化查询；2. 批量处理关系选择，DRM 替代 LLM 剪枝提效</td></tr><tr><td><strong>知识源类型</strong></td><td>仅结构化知识图谱（如 Freebase、Wikidata）</td><td>混合知识源：- 结构化：KG（Wikidata 等）；- 非结构化：Wikipedia 文档、领域文本（如金融财报）</td></tr><tr><td><strong>关键创新点</strong></td><td>1. LLM 动态探索 KG 路径，不依赖预定义检索策略；2. 显式推理路径，支持知识追溯与 KG 修正；3. 多路径 beam search 提升正确答案概率</td><td>1. 跨源检索优化：KG 缩小文本检索范围，文本补充 KG 实体细节；2. 上下文增强实体排序：结合问题、三元组、文本降歧义；3. 迭代深度检索：按实体限制语料规模，降噪声提效</td></tr><tr><td><strong>效率优化手段</strong></td><td>1. ToG-R 随机剪枝实体，减少 LLM 调用；2. 固定 beam width（N=3）与 depth（D=3）平衡性能与成本</td><td>1. 关系剪枝批量处理，减少 LLM 调用；2. DRM（如 BGE-Reranker）替代 LLM 剪枝实体；3. 主题修剪、查询优化减少无效探索</td></tr><tr><td><strong>核心优势</strong></td><td>1. 多跳推理能力强，处理 KG 信息不完整场景；2. 推理透明可追溯，支持错误修正；3. 即插即用，兼容不同 LLM/KG</td><td>1. 深度整合跨源知识，复杂任务（多跳文档 QA、领域推理）性能优；2. 小模型（如 Llama3-8B）结合后可媲美 GPT-3.5；3. 效率高于 ToG，实体剪枝时间仅为 ToG 的 68.7%</td></tr><tr><td><strong>实验核心结果</strong></td><td>1. GPT-4+ToG 在 6/9 数据集（如 WebQSP、GrailQA）达 SOTA；2. Llama2-70B+ToG 媲美 GPT-4 直接推理；3. CWQ 数据集准确率 69.5%，超基线 20%+</td><td>1. GPT-3.5+ToG 2.0 在 6/7 数据集（如 WebQSP、AdvHotpotQA）达 SOTA；2. WebQSP 准确率 81.1%（超 ToG 4.93%），AdvHotpotQA 准确率 42.9%（超 SOTA 5.51%）；3. 金融 ToG-FinQA 准确率 34.0%，远超 GraphRAG（6.2%）</td></tr><tr><td><strong>适用场景</strong></td><td>知识密集型单源推理任务：多跳 KBQA、事实核查、槽填充</td><td>复杂跨源推理任务：多跳文档 QA、领域专属推理（如金融）、需细粒度文本补充的 KG 推理</td></tr></tbody></table><h4 id="对比基线-多轮rag专题" tabindex="-1"><a class="header-anchor" href="#对比基线-多轮rag专题"><span>对比基线：多轮RAG专题</span></a></h4><p>RAT(https://arxiv.org/pdf/2403.05313)、 Interactive-KBQA（https://arxiv.org/abs/2402.15131）、 FLARE （https://aclanthology.org/2023.emnlp-main.495.pdf）</p><p>RAG在上下文管理上面临着问题，而以上论文诞生于多轮RAG即迭代式（iterative）阶段，特点鲜明。</p><table><thead><tr><th style="text-align:left;">维度</th><th style="text-align:left;">FLARE（长文本生成）</th><th style="text-align:left;">Interactive-KBQA（知识图谱问答）</th><th style="text-align:left;">RAT（长周期推理）</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>核心痛点</strong></td><td style="text-align:left;">单轮检索无法覆盖长文本生成的动态信息需求</td><td style="text-align:left;">复杂问题难以单轮解析为结构化逻辑形式</td><td style="text-align:left;">思维链易出现事实幻觉且缺乏外部修正机制</td></tr><tr><td style="text-align:left;"><strong>核心技术</strong></td><td style="text-align:left;">前瞻式主动检索（临时句生成→置信度判断→检索修正）</td><td style="text-align:left;">智能体工具交互（3 类 KB 工具 + 多轮 Thought-Action）</td><td style="text-align:left;">检索增强 CoT 修订（分步推理→逐段检索→迭代修正）</td></tr><tr><td style="text-align:left;"><strong>检索触发逻辑</strong></td><td style="text-align:left;">低置信度 token（概率＜阈值 θ）触发</td><td style="text-align:left;">推理步骤拆解需求触发（如需定位实体 / 关系）</td><td style="text-align:left;">思维链单步推理完成后强制触发</td></tr><tr><td style="text-align:left;"><strong>关键设计亮点</strong></td><td style="text-align:left;">前瞻式查询（对齐未来生成意图）</td><td style="text-align:left;">跨 KB 通用工具 + 人工干预修正</td><td style="text-align:left;">因果式修订（仅依赖已修正历史步骤）</td></tr><tr><td style="text-align:left;"><strong>典型实验提升</strong></td><td style="text-align:left;">2WikiMultihopQA F1 提升 8%-10%</td><td style="text-align:left;">CWQ 数据集 F1 从 36.5% 升至 49.07%</td><td style="text-align:left;">HumanEval pass@1 提升 17%-21%</td></tr></tbody></table><p>2025 年主流的<strong>Agentic RAG</strong>（智能体驱动的自主检索规划）与<strong>Context Engineering</strong>（上下文的系统化设计与优化），已从 “检索策略自主性”“上下文管理精细化”“系统架构扩展性” 三个维度实现技术突破，三篇早期工作的局限性主要体现在以下方面：</p><h5 id="_1-检索决策机制-从-被动触发-到-自主规划-的代差-agentic-rag-视角" tabindex="-1"><a class="header-anchor" href="#_1-检索决策机制-从-被动触发-到-自主规划-的代差-agentic-rag-视角"><span>1. 检索决策机制：从 “被动触发” 到 “自主规划” 的代差（Agentic RAG 视角）</span></a></h5><p>三篇工作的检索决策均依赖<strong>预定义规则</strong>，缺乏 Agentic RAG 的 “自主判断与规划能力”，具体表现为：</p><ul><li><strong>FLARE 的触发逻辑僵化</strong>：仅通过 “token 置信度阈值” 单一指标决定是否检索，无法像 Agentic RAG 那样结合 “任务类型（如摘要 / 问答）、知识新鲜度（如实时数据需求）、历史检索质量” 做综合决策。例如生成 2025 年美国大选摘要时，FLARE 可能因 “候选人最新民调” token 置信度未低于阈值而漏检，而 Agentic RAG 会主动判断 “时效性需求” 并触发检索。</li><li><strong>Interactive-KBQA 的工具选择固化</strong>：强制使用预定义的 3 类 KB 工具，并局限于静态KG</li><li><strong>RAT 的检索流程机械</strong>：对每个思维步骤强制检索，缺乏 Agentic RAG 的 “检索结果评估与重检机制”。例如检索到错误的 Minecraft 合成配方时，RAT 会直接用错误知识修正 CoT，而 Agentic RAG 会评估 “检索结果与任务的相关性”，自动发起二次检索直至获取准确信息。</li></ul><h5 id="_2-上下文管理-从-简单拼接-到-系统化工程-的不足-context-engineering-视角" tabindex="-1"><a class="header-anchor" href="#_2-上下文管理-从-简单拼接-到-系统化工程-的不足-context-engineering-视角"><span>2. 上下文管理：从 “简单拼接” 到 “系统化工程” 的不足（Context Engineering 视角）</span></a></h5><p>三篇工作均将上下文视为 “检索结果的直接拼接”，未践行 Context Engineering 的 “优化、压缩、结构化” 理念，导致上下文效率与质量不足：</p><ul><li><strong>上下文冗余与噪声</strong>：FLARE 将检索到的文档全文嵌入上下文，未做 “相关性筛选与摘要压缩”；RAT 直接拼接多轮检索结果，易超出 LLM 上下文窗口。而现代 Context Engineering 会通过 “智能去重、关键信息提取、分块锚点定位” 优化上下文，例如仅保留 “原木合成木板” 的核心步骤，而非整页 Wiki 内容。</li><li><strong>多模态与多源信息适配缺失</strong>：三篇工作均局限于 “文本类检索结果”，无法像 Context Engineering 那样整合 “表格、图像、数据库数据” 等多模态信息。例如回答 “某产品参数对比” 时，Interactive-KBQA 无法检索并结构化表格数据，只能依赖文本描述，而现代系统可通过多模态处理器将表格转为结构化上下文。</li><li><strong>上下文与指令的协同不足</strong>：三篇工作仅将检索结果作为 “补充知识”，未像 Context Engineering 那样将 “推理思维模板、输出格式要求、用户角色信息” 与知识融合为结构化提示。例如生成专业法律摘要时，无法像现代系统那样在上下文中嵌入 “法律术语规范、判决案例引用格式” 等指令，导致输出质量依赖模型自身能力。</li></ul><h5 id="_3-系统架构-从-单模块耦合-到-多智能体协同-的局限" tabindex="-1"><a class="header-anchor" href="#_3-系统架构-从-单模块耦合-到-多智能体协同-的局限"><span>3. 系统架构：从 “单模块耦合” 到 “多智能体协同” 的局限</span></a></h5><p>三篇工作均为 “单 LLM + 单检索模块” 的耦合架构，无法适配 Agentic RAG 的 “多智能体协作” 需求：</p><ul><li><strong>缺乏专业化分工</strong>：例如处理 “跨领域复杂问题”（如 “分析某药企新药的专利状态与临床数据”）时，RAT 需独自完成 “问题拆解、专利检索、临床数据检索、推理修正” 全流程，而 Agentic RAG 可分配 “专利智能体、医疗数据智能体、推理智能体” 协同工作，效率与准确性显著提升。</li><li><strong>无持久化记忆机制</strong>：三篇工作均为 “无状态系统”，无法像 Agentic RAG 那样通过 “短期对话记忆 + 长期知识记忆” 复用历史信息。例如多轮对话中重复询问 “CBS 持股相关问题” 时，Interactive-KBQA 会重新执行完整检索流程，而 Agentic RAG 可直接调用记忆中的历史检索结果，降低冗余开销。</li></ul><h5 id="_4-技术定位-从-核心方案-到-组件化工具-的角色转变" tabindex="-1"><a class="header-anchor" href="#_4-技术定位-从-核心方案-到-组件化工具-的角色转变"><span>4. 技术定位：从 “核心方案” 到 “组件化工具” 的角色转变</span></a></h5><p>随着<strong>长上下文模型</strong>（如 GPT-4o、Claude 3.5，窗口达 1M tokens）的普及，三篇工作的核心场景已被部分替代：</p><ul><li><strong>FLARE 的长文本生成场景弱化</strong>：长上下文模型可直接读取全文生成摘要，无需 FLARE 的 “分段生成 + 逐句检索” 流程，仅在小模型（如 Mistral-7B）场景仍有价值。</li><li><strong>RAT 的 CoT 修正需求降低</strong>：大模型自身的思维链能力已显著提升，结合长上下文可直接生成更准确的 CoT，RAT 的 “分步检索修正” 仅在低参模型或超复杂推理（如数学定理证明）中必要。</li></ul><h5 id="总结-过时性的本质与价值留存" tabindex="-1"><a class="header-anchor" href="#总结-过时性的本质与价值留存"><span>总结：过时性的本质与价值留存</span></a></h5><p>三篇工作的 “过时性” 并非技术失效，而是<strong>技术定位从 “前沿方案” 退化为 “基础组件”</strong>：</p><ul><li>其 “多轮动态检索” 的核心思想仍是 Agentic RAG 的基础，但检索决策、上下文管理等环节需与 “自主规划、系统化工程、多智能体协作” 深度融合；</li><li>在<strong>小模型落地、静态知识库问答、低资源场景</strong>等领域，三者的设计仍具实用价值 —— 例如 FLARE 的置信度触发逻辑可作为 Agentic RAG 的 “轻量级检索模块”，Interactive-KBQA 的人工干预机制可用于构建高质量 Agent 训练数据。</li></ul><h3 id="_1-17-刘焕勇-ccks报告" tabindex="-1"><a class="header-anchor" href="#_1-17-刘焕勇-ccks报告"><span>1.17 刘焕勇 CCKS报告</span></a></h3><p>https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;mid=2648423217&amp;idx=1&amp;sn=3205a735b04fa23b9c94780682a008a4&amp;chksm=8266620c383f3f2953e8eb80202a0906b99e2fd8480113432f1b028cb7705cd286029665cf3e&amp;scene=0&amp;xtrack=1&amp;subscene=90#rd</p><p><strong>《面向“搜、问、推、写”应用场景的文档解析及知识库建设实践》</strong></p><p>总结为关于知识库及RAG落地的十条建议，是个方法论和方向的指引：</p><ol><li>不要为了上RAG而上RAG，尤其是NL2SQL,KBQA这种类型，之前解决的很好的就不要再折腾了。</li><li>不要为了上变体而上变体，GraphRAG、多模态RAG、DeepResearch等能不上就不上，把最基本RAG做出来就好。</li><li>通用的RAG是一种标品，标品从来都解决不了优化问题，要放弃这种思路。</li><li>RAG本身就是破布，是面向具体业务问题而做的补丁，要有这种意识，面向业务做RAG，而不是面向RAG做业务，具体case 具体分析，评估先行，可用的RAG一定是有很多路由逻辑的。</li><li>目前开源的RAG框架有很多，其意义其实并不是为了生产，而是为了快速做场景验证，要做开源框架祛魅</li><li>能自己动手写就动手写，RAG没多少复杂的东西，开源框架同质化，黑盒化，不利于做问题定位，要适当抛弃；</li><li>RAG本身就是无处不在的，它是一种框架，而不是一种单独的技术，更多时候还是一种工程架构</li><li>决定RAG好不好用的，不是RAG技术本身，而在于用户的问题域是否建模清楚，以及业务实现逻辑的设计。</li><li>落地总是二八原则，很多优化方案都是解决20%长尾问题而设计，这个需要清楚，需要衡量ROI投入产出比；</li><li>RAG的文档解析要做，但并不需要文档解析做到100%还原，这是一条歧路。应该投入，但不要过度关注。文档解析是手段，不是目的。</li></ol><p>另外：</p><p><strong>1、GraphRAG中的“KG”不是“KG”</strong></p><p>GraphRAG里抽取图谱的目的是做索引，做锚点，做关联，能连上就行，所以，不要跟做专门抽取的混为一谈，也可以不抽实体和关系。GraphRAG还是专注在做问答任务，说白了还是RAG，不是ie任务，它背后还是用llm做的抽取、索引和关联。</p><p>为什么graphrag通常做抽取不带schema?，定不出来（业务难），定出来太少（太少bridge的作用不够），定出来太大（抽取难度大，上下文爆炸），也就是没也不好，太少也不好，太多也不好，很主观。</p><p><strong>2、GraphRAG的跳转作用可以用于复杂推理数据合成</strong></p><p>复杂推理任务，核心特征是需要多跳，Graph中的关联关系天然的给出了数据输入。目前，这块是提升大模型推理能力的一个重要方向。</p><p><strong>DeepDive</strong>(https://arxiv.org/pdf/2509.10446)，通过开放知识图谱自动合成含“模糊实体”的复杂问题，构建深度搜索问答数据集。</p><p><strong>MedResearcher-R1</strong>(https://arxiv.org/abs/2508.14880)，通过医学知识图谱生成复杂的多跳问答</p><p>2023年大模型落地，有三驾马车【文档解析+知识图谱+大模型】</p><p>2025年变成新的三驾马车【知识图谱+强化学习+大模型】</p><p>三者的关系是：<strong>通过知识图谱来增强LLM的RL能力，使之能更好地利用知识图谱本身，解决领域知识推理问题</strong>。</p><h3 id="_1-18-rag结合gnn" tabindex="-1"><a class="header-anchor" href="#_1-18-rag结合gnn"><span>1.18 RAG结合GNN</span></a></h3><p>图神经网络GNN：能够结合图的结构信息和文本的语义信息统一表示。</p><p>https://mp.weixin.qq.com/s/2vA2KdD9pgfZmUaVM3GE2w</p><p>《<strong>G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge</strong>》(https://arxiv.org/pdf/2509.24276，https://rmanluo.github.io/gfm-rag/latest/)</p><p>统一了图的接口，定义图的四层结构（节点属性，结构化三元素，非结构化文本，全局分组）</p><p>统一不同知识图谱的图结构要素G=(V,E,R,T,S)</p><p>训练了一个能够阅读图信息的模型</p><h3 id="_1-19-graphrag的缺点与未来优化方向" tabindex="-1"><a class="header-anchor" href="#_1-19-graphrag的缺点与未来优化方向"><span>1.19 GraphRAG的缺点与未来优化方向</span></a></h3><ul><li><p>一般Graph都是离线构建，无法随用随更新？</p><ul><li>简单解决方案：定期更新，知识库数据范围有限</li><li>在回答时进行子问题拆解，过程中判断是否需要实时更新图谱《<strong>StepChain GraphRAG: Reasoning OverKnowledge Graphs for Multi-Hop Question Answering</strong>》(https://arxiv.org/pdf/2510.02827)</li></ul></li><li><p>检索时是一次检索，如果没有找到需要的文档/找到的文档排序不理想？</p><ul><li>现有检索方法，尤其是HippoRAG在常见的多跳问答上已经能做到较好的召回，但是生成效果有限</li><li>采用迭代检索的策略进行补充：迭代思考要进行良好的控制，简单问题容易陷入循环过度思考，复杂问题容易找不到相关的证据文档DeepResearch。针对GraphRAG与对应的迭代式方法，论文《<strong>BEYOND STATIC RETRIEVAL: OPPORTUNITIES AND PITFALLS OF ITERATIVE RETRIEVAL IN GRAPHRAG</strong>》（https://arxiv.org/pdf/2509.25530）提出了“桥梁文档”的概念，即多跳任务的关键文档（本质上是图谱结构中有效的较长的证据链的中间实体与关系对应的三元组）。 <ul><li>推理时的迭代策略：IRCOT，IRGS（根据生成的答案片段寻找缺口），TOG，GCOT</li><li>基于图的检索结构：HippoRAG2，GraphRAG，RAPTOR，GFM-RAG</li><li>两两组合做实验。同时，提出方法BDTR，目的是找到桥梁文档，构造完整证据，并引导检索</li></ul></li></ul></li></ul>',236)]))}const o=e(i,[["render",l]]),h=JSON.parse('{"path":"/llm/RAG.html","title":"RAG方法与思路总结","lang":"zh-CN","frontmatter":{"date":"2025-09-12T00:00:00.000Z","category":["LLM"],"tag":["大模型","RAG","检索增强生成"]},"headers":[{"level":2,"title":"基本流程回顾","slug":"基本流程回顾","link":"#基本流程回顾","children":[]},{"level":2,"title":"1 关键点记录","slug":"_1-关键点记录","link":"#_1-关键点记录","children":[{"level":3,"title":"1.1 代码code的RAG","slug":"_1-1-代码code的rag","link":"#_1-1-代码code的rag","children":[]},{"level":3,"title":"1.2 多模态GraphRAG","slug":"_1-2-多模态graphrag","link":"#_1-2-多模态graphrag","children":[]},{"level":3,"title":"1.3 Deep Research进展","slug":"_1-3-deep-research进展","link":"#_1-3-deep-research进展","children":[]},{"level":3,"title":"1.4 Embedding模型进展","slug":"_1-4-embedding模型进展","link":"#_1-4-embedding模型进展","children":[]},{"level":3,"title":"1.5 文档智能专题","slug":"_1-5-文档智能专题","link":"#_1-5-文档智能专题","children":[]},{"level":3,"title":"1.6 多模态RAG","slug":"_1-6-多模态rag","link":"#_1-6-多模态rag","children":[]},{"level":3,"title":"1.7 prompt工程","slug":"_1-7-prompt工程","link":"#_1-7-prompt工程","children":[]},{"level":3,"title":"1.8 Memory上下文管理","slug":"_1-8-memory上下文管理","link":"#_1-8-memory上下文管理","children":[]},{"level":3,"title":"1.9 RAG+科研：UltraRAG / FlashRAG","slug":"_1-9-rag-科研-ultrarag-flashrag","link":"#_1-9-rag-科研-ultrarag-flashrag","children":[]},{"level":3,"title":"1.10 GraphRAG的应用与落地","slug":"_1-10-graphrag的应用与落地","link":"#_1-10-graphrag的应用与落地","children":[]},{"level":3,"title":"1.11 RAG加速","slug":"_1-11-rag加速","link":"#_1-11-rag加速","children":[]},{"level":3,"title":"1.12 Chunk切分策略","slug":"_1-12-chunk切分策略","link":"#_1-12-chunk切分策略","children":[]},{"level":3,"title":"1.13 RAG+信息论","slug":"_1-13-rag-信息论","link":"#_1-13-rag-信息论","children":[]},{"level":3,"title":"1.14 RAG+强化学习","slug":"_1-14-rag-强化学习","link":"#_1-14-rag-强化学习","children":[]},{"level":3,"title":"1.15 召回策略","slug":"_1-15-召回策略","link":"#_1-15-召回策略","children":[]},{"level":3,"title":"1.16 ToG系列","slug":"_1-16-tog系列","link":"#_1-16-tog系列","children":[]},{"level":3,"title":"1.17 刘焕勇 CCKS报告","slug":"_1-17-刘焕勇-ccks报告","link":"#_1-17-刘焕勇-ccks报告","children":[]},{"level":3,"title":"1.18 RAG结合GNN","slug":"_1-18-rag结合gnn","link":"#_1-18-rag结合gnn","children":[]},{"level":3,"title":"1.19 GraphRAG的缺点与未来优化方向","slug":"_1-19-graphrag的缺点与未来优化方向","link":"#_1-19-graphrag的缺点与未来优化方向","children":[]}]}],"git":{"updatedTime":1761139006000,"contributors":[{"name":"zhengqianhe0","username":"zhengqianhe0","email":"1821984431@qq.com","commits":11,"url":"https://github.com/zhengqianhe0"}],"changelog":[{"hash":"c119e9efed4ae6b489aae2aab4afa529062ca62c","time":1761139006000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"ab2305f756bd327dc62655e986f32036964e1996","time":1760063627000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"c7d539c51ed88f06847331960352ae3f3b9df072","time":1759284793000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"90ec2b63d55c4f5efc060efdc1440497f6b4b19b","time":1758867108000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"b35902d32557271dc568f75344f60260ac141399","time":1758526550000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"c76f1eda618a3e2ddd0466e1677dff9651a3a392","time":1758247453000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"2e31e1b019d4c77e274ab6f47e190a6b93bdc53f","time":1758083519000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"5b6bec5fa3c3bb00b602d6310f99245c1460651a","time":1757989748000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"dd44ec3c59b777d4e324b910d16d5657ca499fc2","time":1757899278000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"76013c7d9d7450d00d90ae1fdc3177df58423740","time":1757818141000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"},{"hash":"2296946e5f674aa5f9d33e39f1ecf2ee16695329","time":1757726171000,"email":"1821984431@qq.com","author":"zhengqianhe0","message":"mryt"}]},"filePathRelative":"llm/RAG.md","excerpt":"\\n<h2>基本流程回顾</h2>\\n<p>知识库-向量化（文本切块）</p>\\n<p>查询-向量化-召回（查询重写，关键词和语义的混合检索，结构化知识表示与召回，召回结果的重排序）</p>\\n<p>上下文学习-生成（预设提示词，temperature）</p>\\n<p>🎉<strong>常看常新：2025年CCF-A主会RAG论文</strong></p>\\n<p>https://www.notion.so/RAG-2025-2270b854c75b808fb90dec88c4ed1140</p>\\n<h2>1 关键点记录</h2>\\n<h3>1.1 代码code的RAG</h3>\\n<p>常见的切分方法：按照函数块切分，按照内部的逻辑块例如循环切分，混合切分并使用部分重叠。</p>"}');export{o as comp,h as data};
